{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-fold Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='logistic', alpha=0.0001, batch_size='auto',\n",
      "       beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(5,), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
      "       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
      "       random_state=None, shuffle=True, solver='lbfgs', tol=0.0001,\n",
      "       validation_fraction=0.1, verbose=False, warm_start=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from math import floor\n",
    "from statistics import mean\n",
    "from numpy import array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Function to scale input vectors\n",
    "def scale(scaler, input_vector):\n",
    "    scaler.fit(input_vector)\n",
    "    scaled_features = scaler.transform(input_vector)\n",
    "    return scaled_features\n",
    "\n",
    "# Function to calculate number of nodes in hidden layer of MLP\n",
    "def calc_node_num(input_len, output_len):\n",
    "    return floor(mean([input_len, output_len]))\n",
    "\n",
    "# Function to construct params dictionary to pass to cross validation\n",
    "def construct_params(node_num):\n",
    "    return {'hidden_layer_sizes': [(node_num,)], \n",
    "            'activation': ['logistic', 'tanh', 'relu'],\n",
    "            'solver': ['lbfgs', 'sgd']}\n",
    "\n",
    "# Function to run 5-fold cross validation (with MLP) to determine optimal hyperparameters for MLP classifer\n",
    "def cross_validation(x_features, y_labels, params):\n",
    "    clf = GridSearchCV(MLPClassifier(), params, cv=3, # Fix this with actual dataset\n",
    "                       scoring='accuracy')\n",
    "    clf.fit(x_features, y_labels)\n",
    "    return clf.best_params_\n",
    "\n",
    "# Return model with given params\n",
    "def get_model(params):\n",
    "    return MLPClassifier(hidden_layer_sizes=params['hidden_layer_sizes'], \n",
    "                         activation=params['activation'], \n",
    "                         solver=params['solver'])\n",
    "\n",
    "# Train model on entire training set\n",
    "def train_model(model, x_vector, y_vector):\n",
    "    model.fit(x_vector, y_vector)\n",
    "    return model\n",
    "\n",
    "# Steps for age and time period classifiers\n",
    "# (0) Scale input/output data (?)\n",
    "scaler = StandardScaler()\n",
    "age_features = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [1, 2, 3, 4, 5], [6, 7, 8, 9, 10]]\n",
    "age_labels = [1, 2, 2, 1, 1, 2]\n",
    "scaled_age_features = scale(scaler, age_features)\n",
    "# print(scaled_age_features)\n",
    "\n",
    "time_period_features = [[1, 2, 3, 4, 5], [6, 7, 8, 9, 10], [6, 7, 8, 9, 10], [6, 7, 8, 9, 10], [6, 7, 8, 9, 10], [6, 7, 8, 9, 10]]\n",
    "time_period_labels = [1, 2, 1, 2, 1, 2]\n",
    "scaled_time_period_features = scale(scaler, time_period_features)\n",
    "# print(scaled_time_period_features)\n",
    "\n",
    "# (1) Perform cross validation with all chosen parameters\n",
    "age_node_num = calc_node_num(len(age_features[0]), len(age_labels))\n",
    "age_params = construct_params(age_node_num)\n",
    "best_age_params = cross_validation(scaled_age_features, age_labels, age_params)\n",
    "\n",
    "tp_node_num = calc_node_num(len(time_period_features[0]), len(time_period_labels))\n",
    "time_period_params = construct_params(tp_node_num)\n",
    "best_tp_params = cross_validation(scaled_time_period_features, time_period_labels, time_period_params)\n",
    "\n",
    "# (2) After determining optimal params based on accuracy, retrain model with those params\n",
    "age_model = get_model(best_age_params)\n",
    "trained_age_model = train_model(age_model, scaled_age_features, age_labels)\n",
    "\n",
    "tp_model = get_model(best_tp_params)\n",
    "trained_tp_model = train_model(tp_model, scaled_time_period_features, time_period_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       1.00      1.00      1.00         1\n",
      "\n",
      "   micro avg       1.00      1.00      1.00         1\n",
      "   macro avg       1.00      1.00      1.00         1\n",
      "weighted avg       1.00      1.00      1.00         1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to predict labels for given test set and return classification report of precision, recall and F1\n",
    "def predict_values(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    print(classification_report(test_labels, predictions))\n",
    "\n",
    "# (3) Split training data into training/test set, use test set to return precision, recall, and f1 scores (?)\n",
    "# test_age_features = [[1, 2, 3, 4, 5]]\n",
    "# test_age_labels = [[2]]\n",
    "# scaled_test_age_features = scale(scaler, test_age_features)\n",
    "# predict_values(trained_age_model, test_age_features, test_age_labels)\n",
    "\n",
    "# test_tp_features = [[6, 7, 8, 9, 10]]\n",
    "# test_tp_labels = [[2]]\n",
    "# scaled_test_tp_features = scale(scaler, test_tp_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (4) Use trained models to predict age and time period of selected authors/novels\n",
    "# test_age_features = [[1, 2, 3, 4, 5]]\n",
    "# test_age_labels = [[1]]\n",
    "# scaled_test_age_features = scale(scaler, test_age_features)\n",
    "\n",
    "# test_tp_features = [[6, 7, 8, 9, 10]]\n",
    "# test_tp_labels = [[2]]\n",
    "# scaled_test_tp_features = scale(scaler, test_tp_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

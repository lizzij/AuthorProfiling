{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[7.49373681e-05, 7.49373681e-05, 1.05411408e-03, 7.61964020e-06,\n",
       "         1.05123114e-05, 3.74686841e-05, 3.74686841e-05, 1.94977690e-05,\n",
       "         1.05123114e-05, 3.74686841e-05, 8.98545755e-06, 1.22433156e-05,\n",
       "         2.24531026e-04, 2.36348449e-05, 2.55729021e-03, 3.15369343e-05,\n",
       "         2.83854487e-03, 6.87301289e-03, 1.18174224e-05, 3.74686841e-05,\n",
       "         4.83096229e-03, 3.79668607e-05, 1.57408067e-03, 2.93207225e-05,\n",
       "         3.74686841e-05, 2.59983294e-04, 2.53112405e-05, 5.49982840e-03,\n",
       "         5.13348831e-03, 3.46959523e-03, 2.84832265e-05, 1.62371384e-03,\n",
       "         1.72534368e-04, 3.74686841e-05, 3.78157518e-05, 2.12713604e-05,\n",
       "         3.58364386e-05, 6.16869451e-04, 2.69563726e-05, 4.27248398e-05,\n",
       "         2.10350119e-04, 3.59249642e-04, 6.66502625e-04, 1.43430363e-04,\n",
       "         2.84832265e-05, 6.38140811e-05, 3.92338425e-04, 3.74686841e-05,\n",
       "         3.52159189e-04, 2.95435561e-04, 1.12406052e-04, 1.52392804e-05,\n",
       "         4.79787351e-04, 1.01157136e-03, 3.16706921e-04, 2.12004558e-03,\n",
       "         1.94977690e-05, 1.65443914e-05, 3.32101955e-05, 2.76527685e-04,\n",
       "         1.47481432e-03, 3.78157518e-05, 6.14505967e-05, 1.25264678e-04,\n",
       "         2.53112405e-05, 1.11556468e-03, 1.77261337e-04, 7.66092975e-05,\n",
       "         4.04155847e-04, 1.53626492e-03, 3.54522673e-04, 6.85767618e-05,\n",
       "         1.15338043e-03, 2.60628644e-05, 1.77261337e-04, 8.03584726e-05,\n",
       "         6.51571611e-06, 7.35043675e-04, 3.15369343e-05, 7.49307353e-05,\n",
       "         1.96169212e-04, 7.61964020e-06, 3.74686841e-05, 5.62509308e-04,\n",
       "         4.96331742e-04, 1.27682163e-05, 2.93207225e-05, 1.06356802e-04,\n",
       "         1.62371384e-03, 9.92663484e-05, 7.09045346e-05, 8.98545755e-06,\n",
       "         1.11083771e-04, 4.04155847e-04, 8.98545755e-06, 4.23521547e-05,\n",
       "         1.79709151e-05, 1.52392804e-05, 3.04785608e-05, 3.19205406e-05,\n",
       "         3.19205406e-05, 1.42416133e-05, 3.37483207e-05, 2.84832265e-05,\n",
       "         3.25881654e-04, 1.22433156e-05, 2.36348449e-05, 4.04155847e-04,\n",
       "         2.43438902e-04, 1.62607733e-03, 1.60716945e-04, 7.09045346e-05,\n",
       "         4.75745563e-03, 2.17318368e-03, 3.05598544e-03, 1.66050977e-05,\n",
       "         7.11408831e-04, 6.32781012e-05, 9.45393795e-06, 1.81279260e-03,\n",
       "         5.93234606e-04, 7.49373681e-05, 1.63080430e-04, 5.06224810e-05,\n",
       "         2.12713604e-05, 5.48410211e-05, 1.23798606e-04, 3.15369343e-05,\n",
       "         2.62346778e-04, 3.30887828e-05, 3.15369343e-05, 7.13772315e-04,\n",
       "         7.04318377e-04, 6.18993031e-05, 6.38140811e-05, 1.18174224e-04,\n",
       "         4.27248398e-05, 2.43438902e-04, 3.73430549e-04, 1.05123114e-05,\n",
       "         5.53055370e-04, 1.05123114e-05, 8.43763962e-04, 2.10246229e-05,\n",
       "         3.78157518e-05, 2.41075418e-04, 1.16283437e-03, 4.25427208e-05,\n",
       "         3.74686841e-05, 3.67299468e-05, 1.84351790e-04, 3.15369343e-05,\n",
       "         9.77357417e-06, 1.91442243e-04, 2.29257995e-04, 3.74686841e-05,\n",
       "         8.27219570e-05, 2.84832265e-05, 3.89955380e-05, 1.65443914e-04,\n",
       "         1.66050977e-05, 1.36660049e-04, 5.86414450e-05, 2.10246229e-05,\n",
       "         5.10728650e-05, 9.77357417e-06, 2.55364325e-05, 2.89949367e-04,\n",
       "         6.64203910e-05, 2.84832265e-05, 3.56886158e-04, 1.15810740e-04,\n",
       "         8.98545755e-06, 2.29257995e-04, 2.32270708e-05, 3.54522673e-05,\n",
       "         2.69563726e-05, 6.51571611e-06, 3.25785806e-05, 2.93072076e-04,\n",
       "         6.74966413e-05, 1.08529838e-04, 3.14343437e-04, 1.42416133e-05,\n",
       "         2.16258831e-03, 1.08720286e-04, 1.30314322e-05, 3.74686841e-05,\n",
       "         3.07252983e-05, 3.79668607e-05, 3.74686841e-05, 1.77261337e-04,\n",
       "         3.14343437e-04, 9.77357417e-06, 2.28050064e-05, 2.12713604e-04,\n",
       "         1.95471483e-05, 3.07252983e-05, 9.77357417e-06, 3.54522673e-05,\n",
       "         3.78268366e-02, 5.60013482e-03, 8.25630987e-02, 1.90560143e-03,\n",
       "         2.33338951e-04, 1.23643717e-01, 7.79092830e-02, 2.83895723e-03,\n",
       "         2.56672846e-03, 0.00000000e+00, 2.11819914e-02, 1.60796464e-01,\n",
       "         2.10912485e-02, 5.13604957e-02, 1.29632750e-04, 2.77414086e-03,\n",
       "         1.29632750e-04, 7.16998743e-02, 3.28748655e-02, 6.46348894e-02,\n",
       "         1.68522576e-03, 1.16669475e-03, 2.94266343e-03, 2.86618011e-02,\n",
       "         1.29632750e-05, 5.16197612e-02, 5.70902633e-02, 1.77078337e-02,\n",
       "         2.61858156e-02, 2.21153472e-02, 1.33651366e-02, 5.22419984e-03,\n",
       "         5.44457552e-03, 3.75934976e-04, 4.60196264e-03, 1.45499799e-01],\n",
       "        [0.00000000e+00, 0.00000000e+00, 6.27652796e-04, 7.35475340e-04,\n",
       "         5.15069274e-06, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "         3.94888567e-04, 1.62124338e-05, 2.65883915e-03, 1.54520782e-05,\n",
       "         2.34964487e-03, 5.56086480e-03, 1.62124338e-05, 0.00000000e+00,\n",
       "         4.91120942e-03, 2.48033791e-05, 1.41627190e-03, 1.91549415e-05,\n",
       "         0.00000000e+00, 3.41619141e-04, 3.72050686e-05, 4.84635969e-03,\n",
       "         5.83416012e-03, 3.41503338e-03, 0.00000000e+00, 1.78684181e-03,\n",
       "         1.36647657e-04, 0.00000000e+00, 3.01088057e-05, 3.93730536e-05,\n",
       "         1.59624512e-05, 6.18388548e-04, 3.08180859e-05, 6.97793007e-06,\n",
       "         2.73295313e-04, 3.77518102e-04, 8.47678683e-04, 1.36418585e-04,\n",
       "         0.00000000e+00, 4.40051775e-05, 7.45771956e-04, 0.00000000e+00,\n",
       "         4.14575094e-04, 3.94888567e-04, 0.00000000e+00, 1.12001321e-05,\n",
       "         4.69002550e-04, 1.45101283e-03, 3.33512925e-04, 2.32416819e-03,\n",
       "         0.00000000e+00, 2.66347127e-05, 8.13596106e-06, 2.02655423e-04,\n",
       "         1.31089108e-03, 2.43186507e-05, 3.01088057e-05, 6.83238283e-05,\n",
       "         2.27364308e-05, 1.47880557e-03, 1.82968896e-04, 3.12800659e-05,\n",
       "         4.75950736e-04, 1.58418639e-03, 2.80243499e-04, 2.98670189e-05,\n",
       "         1.31436517e-03, 3.83098829e-05, 1.50544028e-04, 9.49585410e-05,\n",
       "         1.59624512e-06, 5.90595804e-04, 5.15069274e-06, 6.54460500e-05,\n",
       "         2.18867857e-04, 7.46675472e-06, 0.00000000e+00, 3.39303080e-04,\n",
       "         4.81740891e-04, 9.38401976e-06, 6.86385403e-05, 8.56942931e-05,\n",
       "         1.85169155e-03, 4.97953325e-05, 6.71657973e-05, 2.64155022e-05,\n",
       "         1.40121750e-04, 4.45841930e-04, 1.76103348e-05, 2.55399220e-05,\n",
       "         1.32077511e-05, 1.12001321e-05, 4.48005283e-05, 0.00000000e+00,\n",
       "         2.50240527e-05, 0.00000000e+00, 2.89372756e-05, 0.00000000e+00,\n",
       "         2.83288101e-04, 0.00000000e+00, 2.20025888e-05, 4.02994784e-04,\n",
       "         3.09194274e-04, 1.81463456e-03, 1.86442989e-04, 7.75880762e-05,\n",
       "         9.55327644e-06, 0.00000000e+00, 2.21531328e-03, 0.00000000e+00,\n",
       "         9.13686450e-04, 5.16737064e-05, 5.79015494e-06, 2.17130810e-03,\n",
       "         5.19955914e-04, 0.00000000e+00, 9.26424790e-05, 8.06109820e-05,\n",
       "         1.38963719e-05, 4.34059134e-05, 1.40469571e-04, 2.57534637e-05,\n",
       "         3.02246088e-04, 4.51632085e-05, 2.57534637e-05, 6.61235694e-04,\n",
       "         8.39572466e-04, 2.55399220e-05, 5.21113945e-05, 1.27383409e-04,\n",
       "         0.00000000e+00, 2.39712415e-04, 4.07626908e-04, 0.00000000e+00,\n",
       "         5.33852286e-04, 0.00000000e+00, 1.29004652e-03, 4.37808883e-05,\n",
       "         6.02176114e-05, 1.77178741e-04, 1.22519679e-03, 3.58989606e-05,\n",
       "         0.00000000e+00, 5.99882882e-06, 4.18049187e-04, 1.54520782e-05,\n",
       "         1.27699610e-05, 1.66756462e-04, 2.97613964e-04, 0.00000000e+00,\n",
       "         6.48497353e-05, 0.00000000e+00, 0.00000000e+00, 1.62124338e-04,\n",
       "         0.00000000e+00, 1.00438508e-04, 1.21314629e-04, 3.34795028e-05,\n",
       "         2.18960461e-05, 1.59624512e-05, 2.18960461e-05, 1.24507120e-04,\n",
       "         8.13596106e-06, 0.00000000e+00, 5.33852286e-04, 6.48497353e-05,\n",
       "         0.00000000e+00, 1.85284958e-04, 0.00000000e+00, 2.43186507e-05,\n",
       "         4.40258370e-06, 1.75586963e-05, 1.59624512e-06, 1.81810865e-04,\n",
       "         4.13389651e-05, 0.00000000e+00, 3.64779761e-04, 6.97793007e-06,\n",
       "         1.93275372e-03, 1.08854913e-04, 2.87324122e-05, 0.00000000e+00,\n",
       "         1.04222789e-05, 5.37406547e-05, 0.00000000e+00, 2.28132105e-04,\n",
       "         2.98771995e-04, 2.55399220e-05, 1.27699610e-05, 1.96865268e-04,\n",
       "         1.27699610e-05, 3.58989606e-05, 3.03286573e-05, 4.63212395e-05,\n",
       "         3.83064132e-02, 4.38259411e-03, 7.55203537e-02, 2.58509537e-03,\n",
       "         2.60415013e-04, 1.10987608e-01, 8.28754899e-02, 3.05511271e-03,\n",
       "         2.19129706e-03, 0.00000000e+00, 2.60415013e-02, 1.46448511e-01,\n",
       "         1.72509067e-02, 6.59421625e-02, 8.25706138e-05, 2.98524527e-03,\n",
       "         1.01625371e-04, 7.69177025e-02, 2.72991152e-02, 6.73712692e-02,\n",
       "         1.74668606e-03, 1.22585603e-03, 2.59144695e-03, 3.22215941e-02,\n",
       "         6.35158567e-06, 5.96096315e-02, 5.35692736e-02, 1.77780883e-02,\n",
       "         2.62320488e-02, 2.36723598e-02, 1.60631602e-02, 4.50962583e-03,\n",
       "         5.32898038e-03, 2.41360256e-04, 4.56043851e-03, 1.10822467e-01]]),\n",
       " array([28., 40.]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import numpy\n",
    "from nltk import word_tokenize\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "def clean_and_tokenize_file(file_name):\n",
    "    file = open(file_name)\n",
    "    raw_text= file.read()\n",
    "    words = raw_text.split()\n",
    "    return words\n",
    "\n",
    "def clean_and_tokenize_corpus(directories):\n",
    "    file_list = []\n",
    "    for directory in directories:\n",
    "        for file_name in os.listdir(directory):\n",
    "            if file_name != '.DS_Store':\n",
    "                file = open(directory + file_name)\n",
    "                raw_text = file.read()\n",
    "                tokens = raw_text.split()\n",
    "                file_list.append(tokens)\n",
    "                file.close()\n",
    "    return file_list\n",
    "\n",
    "def count_total_vocab(file_wordfreqs):\n",
    "    vocab = {}\n",
    "    for words in file_wordfreqs:\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def get_corpus_vocab(corpus, unknown_threshold):\n",
    "    initial_dict = count_total_vocab(corpus)\n",
    "    filtered_dict = [word for word, size in initial_dict.items()][:unknown_threshold]\n",
    "    return filtered_dict\n",
    "\n",
    "def feature_vector(novel, vocab, corpus, pos_tags):\n",
    "    tokenized_novel = clean_and_tokenize_file(novel)\n",
    "    word_vector = tf_idf(vocab, tokenized_novel, corpus)\n",
    "    tagged_tokens = nltk.pos_tag(tokenized_novel)\n",
    "    tags = [tag for (word, tag) in tagged_tokens]\n",
    "    num_tokens = len(tokenized_novel)\n",
    "    pos_vector = [tags.count(pos_tag) / num_tokens for pos_tag in pos_tags]\n",
    "    word_vector = word_vector + pos_vector\n",
    "    \n",
    "    # gets ratio of unique words to total # of words\n",
    "    novel_vocab = count_total_vocab([tokenized_novel])\n",
    "    vocab_size = len(novel_vocab) / len(tokenized_novel)\n",
    "    word_vector.append(vocab_size)\n",
    "    \n",
    "    return word_vector\n",
    "\n",
    "def tf_idf(vocab, document, corpus):\n",
    "    counts = Counter(document)\n",
    "    doc_len = len(document)\n",
    "    doc_num = len(corpus)\n",
    "    tf_idfs = []\n",
    "    for word in vocab:\n",
    "        doc_counter = 0\n",
    "        tf = counts[word] / len(document)\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                doc_counter += 1\n",
    "        idf = math.log(doc_num / doc_counter)\n",
    "        tf_idfs.append(tf*idf)\n",
    "    return tf_idfs\n",
    "\n",
    "def create_vector_arrays(training_data, corpus_dict, vocab, corpus, pos_tags):\n",
    "    len_feature_vector = len(vocab) + len(pos_tags) + 1\n",
    "    print(len_feature_vector)\n",
    "    vector_array = numpy.zeros((len(training_data), len_feature_vector))\n",
    "    results_array = numpy.zeros(len(training_data))\n",
    "    index = 0\n",
    "    for data, age in training_data.items():\n",
    "        results_array[index] = age\n",
    "        vector_array[index] = feature_vector(corpus_dict + data, vocab, corpus, pos_tags)\n",
    "        index += 1\n",
    "    return (vector_array, results_array)\n",
    "    \n",
    "    \n",
    "training_data = {'121-0.txt': 28, '158-0.txt':40}\n",
    "corpus = clean_and_tokenize_corpus(['data/JaneAusten/', 'data/CarrollLewis/', 'data/CharlesDickens/'])\n",
    "vocab = get_corpus_vocab(corpus, 200)\n",
    "corpus_dict = 'data/JaneAusten/'    \n",
    "\n",
    "tagset = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "\n",
    "create_vector_arrays(training_data, corpus_dict, vocab, corpus, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from torch import tensor, Size\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import functional\n",
    "from torch import from_numpy\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "input_array = array(features_vector_array[0], dtype='float32')\n",
    "input_vector = from_numpy(input_array)\n",
    "\n",
    "output_array = array(features_vector_array[1], dtype='float32')\n",
    "target_vector = from_numpy(output_array)\n",
    "\n",
    "input_and_target = TensorDataset(input_vector, target_vector)\n",
    "\n",
    "predict_array = array(test_vector, dtype='float32')\n",
    "predict_vector = from_numpy(predict_array)\n",
    "\n",
    "dickens_predict_array = array(dickens_test_array, dtype='float32')\n",
    "dickens_predict_vector = from_numpy(dickens_predict_array)\n",
    "\n",
    "linear_model = nn.Linear(115, 1)\n",
    "loss_func = nn.functional.mse_loss \n",
    "optimize = optim.SGD(linear_model.parameters(), lr=0.001)\n",
    "loss = loss_func(linear_model(input_vector), target_vector)\n",
    "\n",
    "def train(num_epochs, model, loss_func, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in input_and_target:\n",
    "            \n",
    "            # Predictions\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            \n",
    "            # Stochastic radient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    print(loss)\n",
    "\n",
    "train(500, linear_model, loss_func, optimize)\n",
    "\n",
    "pred = linear_model(predict_vector)\n",
    "print(pred)\n",
    "\n",
    "def goodness(predicted, actual, range_size):\n",
    "    return float((predicted - actual)) / float(range_size)\n",
    "\n",
    "print(\"Jane Austen\")\n",
    "print(\"Age predicted for Mansfield Park: %s\" % pred)\n",
    "print(\"Goodness Metric: %s\\n\" % abs(goodness(pred, 39, 15)))\n",
    "\n",
    "pred = linear_model(dickens_predict_vector)\n",
    "print(\"Charles Dickens\")\n",
    "print(\"Age predicted for Oliver Twist: %s\" % pred)\n",
    "print(\"Goodness Metric: %s\" % abs(goodness(pred, 25, 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

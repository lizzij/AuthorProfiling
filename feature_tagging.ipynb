{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import operator\n",
    "import math\n",
    "import readability\n",
    "import random\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import zeros\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "nltk.internals.config_java(options='-Xmx3024m')\n",
    "st = StanfordPOSTagger('english-left3words-distsim.tagger', path_to_jar='stanford-postagger.jar')\n",
    "ner = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz', 'stanford-ner.jar')\n",
    "\n",
    "def train_test_split(directory):\n",
    "    file_names = os.listdir(directory)\n",
    "    random.shuffle(file_names)\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    i = 0\n",
    "    for file_name in file_names:\n",
    "        if file_name != '.DS_Store':\n",
    "            if i < 4:\n",
    "                train_list.append(file_name)\n",
    "                i+= 1\n",
    "            else:\n",
    "                test_list.append(file_name)\n",
    "                i = 0\n",
    "    return (train_list, test_list)\n",
    "\n",
    "def make_training_dicts(filepath, file_list):\n",
    "    age_dir = {}\n",
    "    time_dir = {}\n",
    "    with open(filepath, encoding='latin-1') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row[0] in file_list:\n",
    "                age_dir[row[0]] = row[1]\n",
    "                time_dir[row[0]] = row[2]\n",
    "    return (age_dir, time_dir)\n",
    "\n",
    "def clean_and_tokenize_file(file_name):\n",
    "    file = open(file_name)\n",
    "    raw_text = file.read()\n",
    "    raw_text_lower = raw_text.lower()\n",
    "    caseless_tokens = nltk.word_tokenize(raw_text_lower)\n",
    "    caseless_tokens = [x for x in caseless_tokens if re.match('\\w+', x)]\n",
    "    sentences = nltk.sent_tokenize(raw_text_lower)\n",
    "    raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "    case_tokens = nltk.word_tokenize(raw_text)\n",
    "    sentence_list = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        sentence_list.append([x for x in words if re.match('\\w+', x)])\n",
    "    return (caseless_tokens, sentence_list, raw_sentence_tokens, case_tokens)\n",
    "\n",
    "\n",
    "def clean_and_tokenize_corpus(directory, file_list):\n",
    "    file_dir = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name in file_list:\n",
    "            file_dir[file_name] = clean_and_tokenize_file(directory + file_name)\n",
    "    return file_dir\n",
    "\n",
    "\n",
    "def count_total_vocab(file_wordfreqs):\n",
    "    vocab = {}\n",
    "    for words in file_wordfreqs:\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def get_corpus_vocab(corpus, unknown_threshold):\n",
    "    initial_dict = count_total_vocab(corpus)\n",
    "    sorted_dict = sorted(initial_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    filtered_dict = [key for key, value in sorted_dict[:unknown_threshold]]\n",
    "    return filtered_dict\n",
    "\n",
    "def get_idf_dict(vocab, corpus):\n",
    "    idf_dict = {}\n",
    "    num_docs = len(corpus) \n",
    "    for word in vocab:\n",
    "        df_count = 1\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                df_count += 1\n",
    "        idf_dict[word] = math.log(num_docs / df_count)\n",
    "    return idf_dict\n",
    "\n",
    "def get_named_entities(novel):\n",
    "    named_entities = ner.tag(novel)\n",
    "    entities = []\n",
    "    cur_entity = []\n",
    "    last_tag = 'O'\n",
    "    for entity in named_entities:\n",
    "        if entity[1] == 'O':\n",
    "            if len(cur_entity) > 0:\n",
    "                new_entry = (' '.join(cur_entity), last_tag)\n",
    "                if new_entry not in entities:\n",
    "                    entities.append(new_entry)\n",
    "                cur_entity = []\n",
    "            last_tag = 'O'\n",
    "        elif entity[1] == last_tag:\n",
    "            cur_entity.append(entity[0])\n",
    "        else:\n",
    "            if len(cur_entity) > 0:\n",
    "                new_entry = (' '.join(cur_entity), last_tag)\n",
    "                if new_entry not in entities:\n",
    "                    entities.append(new_entry)\n",
    "                cur_entity = []\n",
    "            cur_entity.append(entity[0])\n",
    "            last_tag = entity[1]\n",
    "    return entities\n",
    "\n",
    "def get_ne_corpus(corpus):\n",
    "    ne_dict = {}\n",
    "    for key, value in corpus.items():\n",
    "        ne_dict[key] = get_named_entities(value[3])\n",
    "    return ne_dict\n",
    "\n",
    "full_list = train_test_split('backup_plan')\n",
    "train_list = full_list[0]\n",
    "test_list = full_list[1]\n",
    "training_data = make_training_dicts('backup.csv', train_list)\n",
    "training_age = training_data[0]\n",
    "training_time = training_data[1]\n",
    "corpus = clean_and_tokenize_corpus('backup_plan/', train_list)\n",
    "ugram_corpus = [value[0] for key, value in corpus.items()]\n",
    "vocab = get_corpus_vocab(ugram_corpus, 500)\n",
    "idf_dict = get_idf_dict(vocab, ugram_corpus)\n",
    "ne_corpus = get_ne_corpus(corpus)\n",
    "ne_vals = [value for key, value in ne_corpus.items()]\n",
    "ne_vocab = get_corpus_vocab(ne_vals, 100)\n",
    "ne_idf_dict = get_idf_dict(ne_vocab, ne_vals)\n",
    "\n",
    "\n",
    "testing_data = make_training_dicts('backup.csv', test_list)\n",
    "testing_age = testing_data[0]\n",
    "testing_time = testing_data[1]\n",
    "test_corpus = clean_and_tokenize_corpus('backup_plan/', test_list)\n",
    "test_ugram_corpus = [value[0] for key, value in test_corpus.items()]\n",
    "test_idf_dict = get_idf_dict(vocab, test_ugram_corpus)\n",
    "test_ne_corpus = get_ne_corpus(test_corpus)\n",
    "test_ne_vals = [value for key, value in test_ne_corpus.items()]\n",
    "test_ne_idf_dict = get_idf_dict(ne_vocab, test_ne_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ugram_corpus' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-5020e25267d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mword2vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_word2vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mugram_corpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'ugram_corpus' is not defined"
     ]
    }
   ],
   "source": [
    "def train_word2vec(corpus):\n",
    "    model = Word2Vec(corpus, size=100)\n",
    "    model.train(corpus, total_examples=len(corpus), epochs=50)\n",
    "    return model\n",
    "\n",
    "word2vec = train_word2vec(ugram_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:30: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:40: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 3.03641341e-03,  2.20256978e-03,  2.13857490e-03,\n",
       "          2.24937200e-03,  1.46901662e-03,  1.22354375e-03,\n",
       "          1.20539595e-03,  1.06307899e-03,  1.03346943e-03,\n",
       "          1.48716442e-03,  7.66028170e-04,  5.19600155e-04,\n",
       "          8.56767168e-04,  5.01452356e-04,  1.04779664e-03,\n",
       "          9.90487796e-04,  6.71468583e-04,  6.53320784e-04,\n",
       "          6.32307542e-04,  7.59342139e-04,  6.90571530e-04,\n",
       "          4.89035440e-04,  5.62581786e-04,  4.68977346e-04,\n",
       "          5.08138387e-04,  3.30480981e-04,  3.90655264e-04,\n",
       "          2.64575814e-04,  4.51784694e-04,  4.53694989e-04,\n",
       "          3.27615539e-04,  1.93894911e-04,  1.71926522e-04,\n",
       "          3.07951685e-04,  2.97050824e-04,  3.17108918e-04,\n",
       "          3.29525834e-04,  3.61045696e-04,  2.76037583e-04,\n",
       "          2.78903025e-04,  3.64866286e-04,  2.79858172e-04,\n",
       "          2.95140529e-04,  3.47673633e-04,  2.54069194e-04,\n",
       "          1.93894911e-04,  2.38786836e-04,  1.95805206e-04,\n",
       "          1.70016227e-04,  2.34011099e-04,  2.51203752e-04,\n",
       "          1.95805206e-04,  2.02491237e-04,  2.07266974e-04,\n",
       "          1.88164027e-04,  2.09177268e-04,  2.27325068e-04,\n",
       "          1.64285343e-04,  1.53778722e-04,  1.18438271e-04,\n",
       "          2.17773595e-04,  2.19683889e-04,  3.43713817e-04,\n",
       "          1.63330196e-04,  1.55689017e-04,  1.10797092e-04,\n",
       "          1.95805206e-04,  1.81477995e-04,  1.50913280e-04,\n",
       "          1.91984616e-04,  1.96760353e-04,  1.39451512e-04,\n",
       "          1.36586070e-04,  8.50081136e-05,  1.43272102e-04,\n",
       "          9.64698818e-05,  1.46137544e-04,  1.22258860e-04,\n",
       "          1.54733870e-04,  1.50913280e-04,  1.98670648e-04,\n",
       "          1.57599312e-04,  1.49958133e-04,  1.36586070e-04,\n",
       "          1.48047838e-04,  1.31810334e-04,  1.31810334e-04,\n",
       "          1.27034597e-04,  8.30978190e-05,  8.40529663e-05,\n",
       "          1.09841945e-04,  8.02323769e-05,  1.24169155e-04,\n",
       "          1.00290471e-04,  1.06976503e-04,  1.08886797e-04,\n",
       "          1.08886797e-04,  8.21426716e-05,  8.88287030e-05,\n",
       "          1.10797092e-04,  9.74250291e-05,  9.16941451e-05,\n",
       "          1.06021355e-04,  8.59632610e-05,  8.21426716e-05,\n",
       "          8.02323769e-05,  8.40529663e-05,  1.06021355e-04,\n",
       "          1.30855186e-04,  9.83801765e-05,  8.50081136e-05,\n",
       "          5.44433986e-05,  9.32471736e-06,  1.14617681e-04,\n",
       "          1.71926522e-05,  1.05066208e-04,  4.87125146e-05,\n",
       "          9.93353238e-05,  5.63536933e-05,  7.92772296e-05,\n",
       "          3.62955991e-05,  7.25911982e-05,  6.11294300e-05,\n",
       "          6.11294300e-05,  3.15198624e-05,  8.78735557e-05,\n",
       "          8.40529663e-05,  9.45595871e-05,  8.30978190e-05,\n",
       "          6.97257561e-05,  9.64698818e-05,  9.55147344e-05,\n",
       "          8.02323769e-05,  6.87706088e-05,  8.30978190e-05,\n",
       "          8.30978190e-05,  8.40529663e-05,  6.87706088e-05,\n",
       "          5.15779566e-05,  4.87125146e-05,  5.34882513e-05,\n",
       "          5.53985460e-05,  3.91610411e-05,  7.25911982e-05,\n",
       "          5.25331039e-05,  4.48919252e-05,  7.83220822e-05,\n",
       "          8.69184083e-05,  6.20845774e-05,  3.05647150e-05,\n",
       "          5.25331039e-05,  5.92191353e-05,  4.58470725e-05,\n",
       "          3.72507464e-05,  4.39367778e-05,  5.73088407e-05,\n",
       "          4.68022199e-05,  4.77573672e-05,  4.01161885e-05,\n",
       "          2.38786836e-05,  4.68022199e-05,  5.44433986e-05,\n",
       "          6.01742827e-05,  4.48919252e-05,  4.01161885e-05,\n",
       "          8.88287030e-05,  5.06228093e-05,  2.86544203e-05,\n",
       "          5.06228093e-05,  0.00000000e+00,  4.48919252e-05,\n",
       "          5.53985460e-05,  5.63536933e-05,  5.44433986e-05,\n",
       "          6.59051668e-05,  9.45595871e-05,  8.11875243e-05,\n",
       "          3.72507464e-05,  5.63536933e-05,  7.64117875e-05,\n",
       "          1.01245619e-04,  5.25331039e-05,  4.68022199e-05,\n",
       "          2.29235363e-05,  5.34882513e-05,  3.82058938e-05,\n",
       "          5.25331039e-05,  5.53985460e-05,  1.39074955e-04,\n",
       "          0.00000000e+00,  0.00000000e+00,  6.59051668e-05,\n",
       "          3.43853044e-05,  1.43272102e-05,  2.10132416e-05,\n",
       "          4.10713358e-05,  4.10713358e-05,  3.34301571e-05,\n",
       "          3.62955991e-05,  3.05647150e-05,  4.20264832e-05,\n",
       "          5.44433986e-05,  3.15198624e-05,  6.68603141e-05,\n",
       "          3.34301571e-05,  3.15198624e-05,  4.87125146e-05,\n",
       "          4.68022199e-05,  3.34301571e-05,  4.20264832e-05,\n",
       "          7.25911982e-05,  2.76992730e-05,  2.67441256e-05,\n",
       "          2.19683889e-05,  3.72507464e-05,  2.67441256e-05,\n",
       "          5.82639880e-05,  4.10713358e-05,  2.29235363e-05,\n",
       "          0.00000000e+00,  2.67441256e-05,  2.29235363e-05,\n",
       "          5.25331039e-05,  0.00000000e+00,  5.15779566e-05,\n",
       "          3.82058938e-05,  5.73088407e-05,  2.38786836e-05,\n",
       "          4.20264832e-05,  3.72507464e-05,  6.49500194e-05,\n",
       "          6.30397247e-05,  2.38786836e-05,  2.38786836e-05,\n",
       "          5.73088407e-05,  0.00000000e+00,  3.24750097e-05,\n",
       "          4.20264832e-05,  3.82058938e-05,  5.63536933e-05,\n",
       "          2.19683889e-05,  5.73088407e-05,  1.14617681e-04,\n",
       "          4.39367778e-05,  4.77573672e-05,  3.43853044e-05,\n",
       "          4.39367778e-05,  3.72507464e-05,  3.34301571e-05,\n",
       "          3.62955991e-05,  5.27458971e-03,  3.43853044e-05,\n",
       "          5.96035520e-06,  3.15198624e-05,  3.10823912e-06,\n",
       "          5.16564118e-05,  3.34301571e-05,  3.62955991e-05,\n",
       "          2.10132416e-05,  2.57889783e-05,  2.48338310e-05,\n",
       "          4.68022199e-05,  5.82639880e-05,  1.81477995e-05,\n",
       "          2.29235363e-05,  9.55147344e-06,  3.53404517e-05,\n",
       "          0.00000000e+00,  0.00000000e+00,  1.71926522e-05,\n",
       "          3.43853044e-05,  3.53404517e-05,  5.25331039e-05,\n",
       "          5.36431968e-05,  3.53404517e-05,  3.15198624e-05,\n",
       "          2.48338310e-05,  0.00000000e+00,  3.05647150e-05,\n",
       "          2.29235363e-05,  4.58470725e-05,  3.53404517e-05,\n",
       "          4.96676619e-05,  1.71926522e-05,  1.41361807e-04,\n",
       "          3.15198624e-05,  4.10713358e-05,  2.67441256e-05,\n",
       "          4.39367778e-05,  2.19683889e-05,  3.53404517e-05,\n",
       "          2.76992730e-05,  4.20264832e-05,  2.10132416e-05,\n",
       "          1.81477995e-05,  0.00000000e+00,  2.76992730e-05,\n",
       "          4.01161885e-05,  2.29235363e-05,  9.94636518e-05,\n",
       "          5.04886131e-05,  1.91029469e-05,  2.00580942e-05,\n",
       "          3.43853044e-05,  1.52823575e-05,  2.57889783e-05,\n",
       "          1.32703436e-05,  2.76992730e-05,  2.76992730e-05,\n",
       "          3.82058938e-05,  5.25331039e-05,  3.72507464e-05,\n",
       "          4.39367778e-05,  3.17885611e-05,  3.22926991e-05,\n",
       "          5.73088407e-06,  3.62955991e-05,  3.43853044e-05,\n",
       "          2.96095677e-05,  2.29235363e-05,  9.55147344e-06,\n",
       "          3.72507464e-05,  9.55147344e-06,  3.82058938e-06,\n",
       "          3.05647150e-05,  1.81477995e-05,  7.15242624e-05,\n",
       "          3.43853044e-05,  2.38786836e-05,  8.74185430e-05,\n",
       "          2.67441256e-05,  3.53404517e-05,  2.19683889e-05,\n",
       "          1.71926522e-05,  1.62375049e-05,  8.34449728e-05,\n",
       "          1.91029469e-05,  2.67441256e-05,  2.48338310e-05,\n",
       "          4.20264832e-05,  2.10132416e-05,  2.86544203e-05,\n",
       "          1.98678507e-05,  3.15198624e-05,  5.96035520e-05,\n",
       "          0.00000000e+00,  3.43853044e-05,  2.76992730e-05,\n",
       "          2.48338310e-05,  2.00580942e-05,  1.43272102e-05,\n",
       "          2.67441256e-05,  2.67441256e-05,  2.38786836e-05,\n",
       "          1.91029469e-05,  1.43272102e-05,  3.43853044e-05,\n",
       "          3.53404517e-05,  1.78810656e-05,  1.05066208e-05,\n",
       "          1.52823575e-05,  2.76992730e-05,  4.01161885e-05,\n",
       "          2.19683889e-05,  3.43853044e-05,  2.86544203e-05,\n",
       "          2.86544203e-05,  1.24169155e-05,  0.00000000e+00,\n",
       "          0.00000000e+00,  3.05647150e-05,  1.71926522e-05,\n",
       "          9.33788982e-05,  0.00000000e+00,  3.05647150e-05,\n",
       "          1.52823575e-05,  6.21647824e-06,  0.00000000e+00,\n",
       "          2.96095677e-05,  2.10132416e-05,  2.57889783e-05,\n",
       "          4.58470725e-05,  0.00000000e+00,  2.76992730e-05,\n",
       "          1.78810656e-05,  1.91029469e-05,  2.96095677e-05,\n",
       "          4.68022199e-05,  1.91029469e-05,  0.00000000e+00,\n",
       "          8.59632610e-06,  1.05066208e-05,  1.71926522e-05,\n",
       "          1.43272102e-05,  0.00000000e+00,  2.29235363e-05,\n",
       "          2.10132416e-05,  7.64117875e-06,  3.05647150e-05,\n",
       "          2.19683889e-05,  1.52823575e-05,  1.81477995e-05,\n",
       "          2.86544203e-05,  2.67441256e-05,  2.10132416e-05,\n",
       "          4.29816305e-05,  0.00000000e+00,  2.57889783e-05,\n",
       "          0.00000000e+00,  3.43853044e-05,  1.43272102e-05,\n",
       "          2.00580942e-05,  1.71926522e-05,  2.19683889e-05,\n",
       "          2.10132416e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "          2.19683889e-05,  9.55147344e-06,  2.29235363e-05,\n",
       "          1.21426285e-04,  7.64117875e-06,  1.81477995e-05,\n",
       "          2.76992730e-05,  2.10132416e-05,  5.69460164e-06,\n",
       "          3.97357014e-06,  2.19683889e-05,  2.76992730e-05,\n",
       "          2.19683889e-05,  1.24169155e-05,  1.43272102e-05,\n",
       "          1.62375049e-05,  1.43272102e-05,  1.05066208e-05,\n",
       "          1.81477995e-05,  2.10132416e-05,  2.10132416e-05,\n",
       "          8.93369053e-06,  3.22926991e-05,  1.81477995e-05,\n",
       "          1.24329565e-05,  1.91029469e-05,  1.71926522e-05,\n",
       "          7.54978326e-05,  1.05066208e-05,  2.29235363e-05,\n",
       "          1.71926522e-05,  0.00000000e+00,  2.19683889e-05,\n",
       "          6.15903371e-05,  3.05647150e-05,  0.00000000e+00,\n",
       "          2.57889783e-05,  1.52823575e-05,  9.32471736e-06,\n",
       "          1.14617681e-05,  1.14617681e-05,  3.15198624e-05,\n",
       "          0.00000000e+00,  1.24169155e-05,  2.38786836e-05,\n",
       "          6.68603141e-06,  0.00000000e+00,  0.00000000e+00,\n",
       "          8.59632610e-06,  2.57889783e-05,  2.67441256e-05,\n",
       "          1.19207104e-05,  2.67441256e-05,  1.81477995e-05,\n",
       "          1.14617681e-05,  1.07204286e-03,  0.00000000e+00,\n",
       "          2.10132416e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  2.10132416e-05,  2.19683889e-05,\n",
       "          8.59632610e-06,  0.00000000e+00,  6.68603141e-06,\n",
       "          1.81477995e-05,  1.91029469e-05,  0.00000000e+00,\n",
       "          1.91029469e-05,  1.14617681e-05,  2.10132416e-05,\n",
       "          2.57889783e-05,  0.00000000e+00,  1.52823575e-05,\n",
       "          1.05066208e-05,  2.57889783e-05,  1.71926522e-05,\n",
       "          5.96035520e-05,  2.19683889e-05,  5.36431968e-05,\n",
       "          1.33720628e-05,  1.04079673e-04,  3.57754833e-02,\n",
       "          9.97363796e-03,  7.34292619e-02,  1.90026362e-03,\n",
       "          2.15289982e-03,  1.06887083e-01,  5.69969244e-02,\n",
       "          2.77899824e-03,  2.14191564e-03,  0.00000000e+00,\n",
       "          1.94200351e-02,  1.11170914e-01,  2.30338313e-02,\n",
       "          3.30184534e-02,  7.46924429e-04,  2.32864675e-03,\n",
       "          4.65729350e-03,  7.39125659e-02,  3.08106327e-02,\n",
       "          6.77943761e-02,  1.82337434e-03,  1.03251318e-03,\n",
       "          2.83391916e-03,  2.45935852e-02,  2.63620387e-03,\n",
       "          4.14433216e-02,  5.10654657e-02,  1.55206503e-02,\n",
       "          2.49231107e-02,  1.70035149e-02,  9.31458699e-03,\n",
       "          5.08567663e-03,  4.49253076e-03,  3.18541301e-04,\n",
       "          4.78910369e-03, -5.41949794e-02,  6.21376559e-02,\n",
       "          3.42119783e-01,  2.19753250e-01,  2.12096527e-01,\n",
       "         -1.21650390e-01,  1.61313415e-01, -2.08756831e-02,\n",
       "          2.63753742e-01,  4.53429967e-01, -4.74095605e-02,\n",
       "         -2.39840239e-01,  1.66049991e-02,  1.46805555e-01,\n",
       "          9.17105973e-02,  1.18594207e-01,  9.28815454e-02,\n",
       "         -3.20633620e-01, -4.28077608e-01, -4.42723423e-01,\n",
       "         -2.14883372e-01,  4.44511808e-02, -1.87920451e-01,\n",
       "          1.09072410e-01, -3.39867687e-03, -2.89578259e-01,\n",
       "          2.05009624e-01,  5.75517654e-01, -1.05288684e-01,\n",
       "         -1.78862121e-02, -1.35341093e-01,  8.41727033e-02,\n",
       "          9.58678722e-02,  4.18558568e-01, -1.50838405e-01,\n",
       "          5.84528930e-02,  2.28060633e-01, -3.70254397e-01,\n",
       "          6.69501647e-02, -2.55565166e-01, -2.69189000e-01,\n",
       "          1.65665790e-01, -1.33794904e-01, -6.52509276e-03,\n",
       "         -5.47054768e-01, -1.16979703e-01,  1.64180100e-01,\n",
       "         -1.35319112e-02,  2.68996269e-01,  1.91049695e-01,\n",
       "         -1.27307549e-01, -1.00424983e-01,  2.97726621e-03,\n",
       "         -1.37936115e-01, -2.18529448e-01,  1.15020975e-01,\n",
       "         -4.80421335e-02,  1.60982490e-01, -2.34306842e-01,\n",
       "         -2.17646044e-02,  3.35629284e-01, -1.49867520e-01,\n",
       "         -5.33628702e-01,  2.95003504e-01,  2.41202697e-01,\n",
       "          4.73666564e-02,  3.26177180e-02, -6.77976906e-02,\n",
       "          1.15991615e-01, -1.43006565e-02,  1.58133954e-01,\n",
       "         -1.58438623e-01,  4.08742189e-01,  4.93132174e-01,\n",
       "         -7.81443194e-02, -1.32008433e-01, -2.78527774e-02,\n",
       "          1.44513277e-02,  1.13744572e-01, -1.75666243e-01,\n",
       "         -1.70134023e-01, -1.28760979e-01, -6.14819303e-02,\n",
       "         -8.80364180e-02, -1.33790955e-01,  2.00876743e-01,\n",
       "          2.11242633e-03, -2.06466131e-02, -1.07925095e-01,\n",
       "          7.04416335e-02, -1.28222734e-01,  1.63401678e-01,\n",
       "         -6.93933845e-01,  2.26368718e-02,  4.00115661e-02,\n",
       "         -2.44985491e-01, -3.27896118e-01, -8.45682397e-02,\n",
       "         -2.34232828e-01,  1.40331581e-01,  8.40336134e-02,\n",
       "          3.32509696e-01,  6.46399472e+01,  4.09436310e-04,\n",
       "          4.09436310e-04,  2.44106493e-03,  2.44106493e-03,\n",
       "          2.44106493e-03,  2.44106493e-03,  2.44106493e-03,\n",
       "          2.44106493e-03,  0.00000000e+00,  3.82954243e-03,\n",
       "          3.82954243e-03,  3.82954243e-03,  0.00000000e+00,\n",
       "          0.00000000e+00,  4.68120365e-03,  4.68120365e-03,\n",
       "          4.68120365e-03,  4.68120365e-03,  4.68120365e-03,\n",
       "          4.68120365e-03,  4.68120365e-03,  4.68120365e-03,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          5.68850507e-03,  5.68850507e-03,  5.68850507e-03,\n",
       "          5.68850507e-03,  5.68850507e-03,  5.68850507e-03,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          6.92134237e-03,  6.92134237e-03,  6.92134237e-03,\n",
       "          6.92134237e-03,  6.92134237e-03,  6.92134237e-03,\n",
       "          6.92134237e-03,  6.92134237e-03,  6.92134237e-03,\n",
       "          6.92134237e-03,  6.92134237e-03,  6.92134237e-03,\n",
       "          6.92134237e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), array([28.]))"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_and_tokenize_file(file_name):\n",
    "    file = open(file_name)\n",
    "    raw_text = file.read()\n",
    "    raw_text_lower = raw_text.lower()\n",
    "    caseless_tokens = nltk.word_tokenize(raw_text_lower)\n",
    "    sentences = nltk.sent_tokenize(raw_text_lower)\n",
    "    raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "    case_tokens = nltk.word_tokenize(raw_text)\n",
    "    sentence_list = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        sentence_list.append([x for x in words if re.match('\\w+', x)])\n",
    "    return (caseless_tokens, sentence_list, raw_sentence_tokens, case_tokens)\n",
    "\n",
    "def tf_idf(vocab, document, idf_dict):\n",
    "    counts = Counter(document)\n",
    "    doc_len = len(document)\n",
    "    tf_idfs = []\n",
    "    for word in vocab:\n",
    "        doc_counter = 0\n",
    "        tf = counts[word] / len(document)\n",
    "        idf = idf_dict[word]\n",
    "        tf_idfs.append(tf*idf)\n",
    "    return tf_idfs\n",
    "\n",
    "def get_avg_word2vec(novel):\n",
    "    vec_list = []\n",
    "    for word in novel:\n",
    "        if word in word2vec:\n",
    "            vec_list.append(word2vec[word])\n",
    "    vector_array = array(vec_list)\n",
    "    vector_array = numpy.mean(vector_array, axis=0)\n",
    "    return vector_array.tolist()\n",
    "\n",
    "def get_pairwise_similarity(sentences):\n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        vec_list = []\n",
    "        for word in sentence:\n",
    "            if word in word2vec:\n",
    "                vec_list.append(word2vec[word])\n",
    "        if len(vec_list) > 0:\n",
    "            vector_array = array(vec_list)\n",
    "            vector_array = numpy.mean(vector_array, axis=0)\n",
    "            sentence_vectors.append(vector_array)\n",
    "    cs_sim = cosine_similarity(sentence_vectors, sentence_vectors)\n",
    "    return numpy.mean(cs_sim)\n",
    "\n",
    "def get_pos_vector(novel, pos_tags):\n",
    "    tagged_tokens = st.tag(novel)\n",
    "    tags = [tag for (word, tag) in tagged_tokens]\n",
    "    num_tokens = len(novel)\n",
    "    pos_vector = [tags.count(pos_tag) / num_tokens for pos_tag in pos_tags]\n",
    "    return pos_vector\n",
    "\n",
    "def get_readability_score(sentences):\n",
    "    newline_regex = re.compile(r'\\n')\n",
    "    clean_sents = []\n",
    "    for sentence in sentences:\n",
    "        clean_sents.append(re.sub(newline_regex, ' ', sentence))\n",
    "    readability_text = '\\n'.join(clean_sents)\n",
    "    readability_results = readability.getmeasures(readability_text, lang='en')\n",
    "    flesch_score = readability_results['readability grades']['FleschReadingEase']   \n",
    "    return flesch_score\n",
    "\n",
    "def feature_vector(novel, vocab, idf_dict, pos_tags):\n",
    "    tokens = novel[0] # for unigrams\n",
    "    token_sents = novel[1] #  for sent similarity\n",
    "    case_tokens = novel[3] # for pos tagging and ner\n",
    "    \n",
    "    unigram_vector = tf_idf(vocab, tokens, idf_dict)\n",
    "    pos_vector = get_pos_vector(case_tokens, pos_tags)\n",
    "    word_2_vec_vector = get_avg_word2vec(tokens)\n",
    "    word_vector = unigram_vector + pos_vector + word_2_vec_vector\n",
    "    \n",
    "    # gets ratio of unique words to total # of words\n",
    "    novel_vocab = count_total_vocab([tokens])\n",
    "    vocab_size = len(novel_vocab) / len(tokens)\n",
    "    word_vector.append(vocab_size)\n",
    "      \n",
    "    cs_similarity = get_pairwise_similarity(token_sents)\n",
    "    word_vector.append(cs_similarity)\n",
    "    return word_vector\n",
    "\n",
    "def feature_vector_time_period(novel, vocab, idf_dict, pos_tags, ne_vocab, ne_novel, ne_idf_dict):\n",
    "    word_vector = feature_vector(novel, vocab, idf_dict, pos_tags)\n",
    "    raw_sents = novel[2]\n",
    "    word_vector.append(get_readability_score(raw_sents))\n",
    "    ne_vector = tf_idf(ne_vocab, ne_novel, ne_idf_dict)\n",
    "    time_period_vector = word_vector + ne_vector\n",
    "    return time_period_vector\n",
    "\n",
    "def create_vector_arrays(training_data, corpus, vocab, idf_dict, pos_tags, is_time, ner_vocab, ner_corpus, ner_idf_dict):\n",
    "    if is_time:\n",
    "        len_feature_vector = len(vocab) + len(pos_tags) + len(ner_vocab) + 103\n",
    "    else:\n",
    "        len_feature_vector = len(vocab) + len(pos_tags) + 102\n",
    "    vector_array = zeros((len(training_data), len_feature_vector))\n",
    "    results_array = zeros(len(training_data))\n",
    "    index = 0\n",
    "    for data, age in training_data.items():\n",
    "        results_array[index] = age\n",
    "        novel = corpus[data]\n",
    "        if is_time:\n",
    "            vector_array[index] = feature_vector_time_period(novel, vocab, idf_dict, pos_tags, ner_vocab, ner_corpus[data], ner_idf_dict)\n",
    "        else:\n",
    "            vector_array[index] = feature_vector(novel, vocab, idf_dict, pos_tags)\n",
    "        index += 1\n",
    "    return (vector_array, results_array)\n",
    "    \n",
    "    \n",
    "tagset = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "\n",
    "training_data_time = create_vector_arrays(training_time, corpus, vocab, idf_dict, tagset, True, ne_vocab, ne_corpus, ne_idf_dict)\n",
    "training_data_age = create_vector_arrays(training_age, corpus, vocab, idf_dict, pos_tags, False, None, None, None)\n",
    "testing_data_age = create_vector_arrays(training_time, corpus, vocab, idf_dict, tagset, True, ne_vocab, ne_corpus, ne_idf_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = array(features_vector_array[0], dtype='float32')\n",
    "input_vector = from_numpy(input_array)\n",
    "\n",
    "output_array = array(features_vector_array[1], dtype='float32')\n",
    "target_vector = from_numpy(output_array)\n",
    "\n",
    "input_and_target = TensorDataset(input_vector, target_vector)\n",
    "\n",
    "predict_array = array(test_vector, dtype='float32')\n",
    "predict_vector = from_numpy(predict_array)\n",
    "\n",
    "dickens_predict_array = array(dickens_test_array, dtype='float32')\n",
    "dickens_predict_vector = from_numpy(dickens_predict_array)\n",
    "\n",
    "linear_model = nn.Linear(115, 1)\n",
    "loss_func = nn.functional.mse_loss \n",
    "optimize = optim.SGD(linear_model.parameters(), lr=0.001)\n",
    "loss = loss_func(linear_model(input_vector), target_vector)\n",
    "\n",
    "def train(num_epochs, model, loss_func, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in input_and_target:\n",
    "            \n",
    "            # Predictions\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            \n",
    "            # Stochastic radient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    print(loss)\n",
    "\n",
    "train(500, linear_model, loss_func, optimize)\n",
    "\n",
    "pred = linear_model(predict_vector)\n",
    "print(pred)\n",
    "\n",
    "def goodness(predicted, actual, range_size):\n",
    "    return float((predicted - actual)) / float(range_size)\n",
    "\n",
    "print(\"Jane Austen\")\n",
    "print(\"Age predicted for Mansfield Park: %s\" % pred)\n",
    "print(\"Goodness Metric: %s\\n\" % abs(goodness(pred, 39, 15)))\n",
    "\n",
    "pred = linear_model(dickens_predict_vector)\n",
    "print(\"Charles Dickens\")\n",
    "print(\"Age predicted for Oliver Twist: %s\" % pred)\n",
    "print(\"Goodness Metric: %s\" % abs(goodness(pred, 25, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

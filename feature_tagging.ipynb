{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import operator\n",
    "import math\n",
    "import readability\n",
    "import random\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import zeros\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "# java_path = \"C:/Program Files/Java/jdk1.8.0_144/bin\"\n",
    "# os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "# nltk.internals.config_java(\"C:/Program Files/Java/jdk1.8.0_144/bin\", options='-Xmx3024m')\n",
    "\n",
    "\n",
    "st = StanfordPOSTagger('english-left3words-distsim.tagger', path_to_jar='stanford-postagger.jar')\n",
    "ner = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz', 'stanford-ner.jar')\n",
    "\n",
    "def train_test_split(directory):\n",
    "    file_names = os.listdir(directory)\n",
    "    random.shuffle(file_names)\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    i = 0\n",
    "    for file_name in file_names:\n",
    "        if file_name != '.DS_Store':\n",
    "            if i < 4:\n",
    "                train_list.append(file_name)\n",
    "                i+= 1\n",
    "            else:\n",
    "                test_list.append(file_name)\n",
    "                i = 0\n",
    "    return (train_list, test_list)\n",
    "\n",
    "def make_training_dicts(filepath, file_list):\n",
    "    age_dir = {}\n",
    "    time_dir = {}\n",
    "    with open(filepath, encoding='latin-1') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row[0] in file_list:\n",
    "                age_dir[row[0]] = row[1]\n",
    "                time_dir[row[0]] = row[2]\n",
    "    return (age_dir, time_dir)\n",
    "\n",
    "def clean_and_tokenize_file(file_name):\n",
    "    file = open(file_name, encoding='latin-1')\n",
    "    raw_text = file.read()\n",
    "    raw_text_lower = raw_text.lower()\n",
    "    caseless_tokens = nltk.word_tokenize(raw_text_lower)\n",
    "    caseless_tokens = [x for x in caseless_tokens if re.match('\\w+', x)]\n",
    "    sentences = nltk.sent_tokenize(raw_text_lower)\n",
    "    raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "    case_tokens = nltk.word_tokenize(raw_text)\n",
    "    sentence_list = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        sentence_list.append([x for x in words if re.match('\\w+', x)])\n",
    "    return (caseless_tokens, sentence_list, raw_sentence_tokens, case_tokens)\n",
    "\n",
    "\n",
    "def clean_and_tokenize_corpus(directory, file_list):\n",
    "    file_dir = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name in file_list:\n",
    "            file_dir[file_name] = clean_and_tokenize_file(directory + file_name)\n",
    "    return file_dir\n",
    "\n",
    "\n",
    "def count_total_vocab(file_wordfreqs):\n",
    "    vocab = {}\n",
    "    for words in file_wordfreqs:\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def get_corpus_vocab(corpus, unknown_threshold):\n",
    "    initial_dict = count_total_vocab(corpus)\n",
    "    sorted_dict = sorted(initial_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    filtered_dict = [key for key, value in sorted_dict[:unknown_threshold]]\n",
    "    return filtered_dict\n",
    "\n",
    "def get_idf_dict(vocab, corpus):\n",
    "    idf_dict = {}\n",
    "    num_docs = len(corpus) \n",
    "    for word in vocab:\n",
    "        df_count = 1\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                df_count += 1\n",
    "        idf_dict[word] = math.log(num_docs / df_count)\n",
    "    return idf_dict\n",
    "\n",
    "def get_named_entities(novel):\n",
    "    named_entities = ner.tag(novel)\n",
    "    entities = []\n",
    "    cur_entity = []\n",
    "    last_tag = 'O'\n",
    "    for entity in named_entities:\n",
    "        if entity[1] == 'O':\n",
    "            if len(cur_entity) > 0:\n",
    "                new_entry = (' '.join(cur_entity), last_tag)\n",
    "                if new_entry not in entities:\n",
    "                    entities.append(new_entry)\n",
    "                cur_entity = []\n",
    "            last_tag = 'O'\n",
    "        elif entity[1] == last_tag:\n",
    "            cur_entity.append(entity[0])\n",
    "        else:\n",
    "            if len(cur_entity) > 0:\n",
    "                new_entry = (' '.join(cur_entity), last_tag)\n",
    "                if new_entry not in entities:\n",
    "                    entities.append(new_entry)\n",
    "                cur_entity = []\n",
    "            cur_entity.append(entity[0])\n",
    "            last_tag = entity[1]\n",
    "    return entities\n",
    "\n",
    "def get_ne_corpus(corpus):\n",
    "    ne_dict = {}\n",
    "    for key, value in corpus.items():\n",
    "        ne_dict[key] = get_named_entities(value[3])\n",
    "    return ne_dict\n",
    "\n",
    "full_list = train_test_split('chunk_50/clean')\n",
    "train_list = full_list[0]\n",
    "test_list = full_list[1]\n",
    "training_data = make_training_dicts('chunk_50/clean.csv', train_list)\n",
    "training_age = training_data[0]\n",
    "training_time = training_data[1]\n",
    "corpus = clean_and_tokenize_corpus('chunk_50/clean/', train_list)\n",
    "ugram_corpus = [value[0] for key, value in corpus.items()]\n",
    "vocab = get_corpus_vocab(ugram_corpus, 500)\n",
    "idf_dict = get_idf_dict(vocab, ugram_corpus)\n",
    "ne_corpus = get_ne_corpus(corpus)\n",
    "ne_vals = [value for key, value in ne_corpus.items()]\n",
    "ne_vocab = get_corpus_vocab(ne_vals, 100)\n",
    "ne_idf_dict = get_idf_dict(ne_vocab, ne_vals)\n",
    "\n",
    "\n",
    "testing_data = make_training_dicts('chunk_50/clean.csv', test_list)\n",
    "testing_age = testing_data[0]\n",
    "testing_time = testing_data[1]\n",
    "test_corpus = clean_and_tokenize_corpus('chunk_50/clean/', test_list)\n",
    "test_ugram_corpus = [value[0] for key, value in test_corpus.items()]\n",
    "\n",
    "test_idf_dict = get_idf_dict(vocab, test_ugram_corpus)\n",
    "test_ne_corpus = get_ne_corpus(test_corpus)\n",
    "test_ne_vals = [value for key, value in test_ne_corpus.items()]\n",
    "test_ne_idf_dict = get_idf_dict(ne_vocab, test_ne_vals)\n",
    "print('Finished corpus-wide work')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(corpus):\n",
    "    model = Word2Vec(corpus, size=100)\n",
    "    model.train(corpus, total_examples=len(corpus), epochs=50)\n",
    "    return model\n",
    "\n",
    "word2vec = train_word2vec(ugram_corpus)\n",
    "print('Finished word2vec training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from gensim import corpora\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def lemmatize_text(text):\n",
    "    lemmatized = [WordNetLemmatizer().lemmatize(word) for word in text if word not in stop_words]\n",
    "    return lemmatized\n",
    "\n",
    "def lemmatize_corpus(corpus):\n",
    "    clean_texts = [lemmatize_text(text) for text in corpus]\n",
    "    return clean_texts\n",
    "\n",
    "def get_working_dict(corpus):\n",
    "    return corpora.Dictionary(corpus)\n",
    "    \n",
    "def train_lda(lem_corpus, dictionary):\n",
    "    corpus = [dictionary.doc2bow(text) for text in lem_corpus]\n",
    "    lda_model = LdaModel(corpus, num_topics = 10, id2word=dictionary, passes=15)\n",
    "    return lda_model\n",
    "\n",
    "lemmatized_corpus = lemmatize_corpus(ugram_corpus)\n",
    "dictionary = get_working_dict(lemmatized_corpus)\n",
    "lda_model = train_lda(lemmatized_corpus, dictionary)\n",
    "print('Finished topic-model training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_name_feature_vector(is_time):\n",
    "    tagset = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "    topic_list = []\n",
    "    for i in range(10):\n",
    "        topic_list.append('topic ' + str(i))\n",
    "    init_list = vocab + tagset + ['w2v'] * 100 + topic_list\n",
    "    init_list.append('vocab size')\n",
    "    init_list.append('cs similarity')\n",
    "    if is_time:\n",
    "        time_list = init_list + ne_vocab\n",
    "        time_list.append('flesch score')\n",
    "        return time_list\n",
    "    else:\n",
    "        return init_list\n",
    "    \n",
    "time_feature_names = create_name_feature_vector(True)\n",
    "age_feature_names = create_name_feature_vector(False)\n",
    "print('Finished init vector names')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = StanfordPOSTagger('english-left3words-distsim.tagger', path_to_jar='stanford-postagger.jar')\n",
    "st.java_options = '-mx4g'\n",
    "def clean_and_tokenize_file(file_name):\n",
    "    file = open(file_name)\n",
    "    raw_text = file.read()\n",
    "    raw_text_lower = raw_text.lower()\n",
    "    caseless_tokens = nltk.word_tokenize(raw_text_lower)\n",
    "    sentences = nltk.sent_tokenize(raw_text_lower)\n",
    "    raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "    case_tokens = nltk.word_tokenize(raw_text)\n",
    "    sentence_list = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        sentence_list.append([x for x in words if re.match('\\w+', x)])\n",
    "    return (caseless_tokens, sentence_list, raw_sentence_tokens, case_tokens)\n",
    "\n",
    "def tf_idf(vocab, document, idf_dict):\n",
    "    counts = Counter(document)\n",
    "    doc_len = len(document)\n",
    "    tf_idfs = []\n",
    "    for word in vocab:\n",
    "        doc_counter = 0\n",
    "        tf = counts[word] / len(document)\n",
    "        idf = idf_dict[word]\n",
    "        tf_idfs.append(tf*idf)\n",
    "    return tf_idfs\n",
    "\n",
    "def get_avg_word2vec(novel):\n",
    "    vec_list = []\n",
    "    for word in novel:\n",
    "        if word in word2vec:\n",
    "            vec_list.append(word2vec[word])\n",
    "    vector_array = array(vec_list)\n",
    "    vector_array = mean(vector_array, axis=0)\n",
    "    return vector_array.tolist()\n",
    "\n",
    "def get_pairwise_similarity(sentences):\n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        vec_list = []\n",
    "        for word in sentence:\n",
    "            if word in word2vec:\n",
    "                vec_list.append(word2vec[word])\n",
    "        if len(vec_list) > 0:\n",
    "            vector_array = array(vec_list)\n",
    "            vector_array = mean(vector_array, axis=0)\n",
    "            sentence_vectors.append(vector_array)\n",
    "    cs_sim = cosine_similarity(sentence_vectors, sentence_vectors)\n",
    "    return mean(cs_sim)\n",
    "\n",
    "def get_pos_vector(novel, pos_tags):\n",
    "    tagged_tokens = st.tag(novel)\n",
    "    tags = [tag for (word, tag) in tagged_tokens]\n",
    "    num_tokens = len(novel)\n",
    "    pos_vector = [tags.count(pos_tag) / num_tokens for pos_tag in pos_tags]\n",
    "    return pos_vector\n",
    "\n",
    "# gets topic ids of top-ten topics\n",
    "def get_topic_vector(novel):\n",
    "    lem_novel = lemmatize_text(novel)\n",
    "    new_text = dictionary.doc2bow(novel)\n",
    "    suggested_topics = dict(lda_model.get_document_topics(new_text))\n",
    "    topics = [suggested_topics[i] if i in suggested_topics else 0 for i in range(10)]\n",
    "    return topics\n",
    "\n",
    "def get_readability_score(sentences):\n",
    "    newline_regex = re.compile(r'\\n')\n",
    "    clean_sents = []\n",
    "    for sentence in sentences:\n",
    "        clean_sents.append(re.sub(newline_regex, ' ', sentence))\n",
    "    readability_text = '\\n'.join(clean_sents)\n",
    "    readability_results = readability.getmeasures(readability_text, lang='en')\n",
    "    flesch_score = readability_results['readability grades']['FleschReadingEase']   \n",
    "    return flesch_score\n",
    "\n",
    "def feature_vector(novel, vocab, idf_dict, pos_tags):\n",
    "    tokens = novel[0] # for unigrams\n",
    "    token_sents = novel[1] #  for sent similarity\n",
    "    case_tokens = novel[3] # for pos tagging and ner\n",
    "    \n",
    "    unigram_vector = tf_idf(vocab, tokens, idf_dict)\n",
    "    pos_vector = get_pos_vector(case_tokens, pos_tags)\n",
    "    word_2_vec_vector = get_avg_word2vec(tokens)\n",
    "    topic_vector = get_topic_vector(tokens)\n",
    "    word_vector = unigram_vector + pos_vector + word_2_vec_vector + topic_vector\n",
    "    \n",
    "    # gets ratio of unique words to total # of words\n",
    "    novel_vocab = count_total_vocab([tokens])\n",
    "    vocab_size = len(novel_vocab) / len(tokens)\n",
    "    word_vector.append(vocab_size)\n",
    "      \n",
    "    cs_similarity = get_pairwise_similarity(token_sents)\n",
    "    word_vector.append(cs_similarity)\n",
    "    return word_vector\n",
    "\n",
    "def feature_vector_time_period(age_vector, novel, ne_vocab, ne_novel, ne_idf_dict):\n",
    "    raw_sents = novel[2]\n",
    "    ne_vector = tf_idf(ne_vocab, ne_novel, ne_idf_dict)\n",
    "    time_period_vector = age_vector.tolist() + ne_vector\n",
    "    time_period_vector.append(get_readability_score(raw_sents))\n",
    "    return time_period_vector\n",
    "\n",
    "def create_age_vector_arrays(training_data, corpus, vocab, idf_dict, pos_tags):\n",
    "    len_feature_vector = len(vocab) + len(pos_tags) + 112\n",
    "    vector_array = zeros((len(training_data), len_feature_vector))\n",
    "    results_array = zeros(len(training_data))\n",
    "    index = 0\n",
    "    for data, age in training_data.items():\n",
    "        results_array[index] = age\n",
    "        novel = corpus[data]\n",
    "        vector_array[index] = feature_vector(novel, vocab, idf_dict, pos_tags)\n",
    "        index += 1\n",
    "    return (vector_array, results_array)\n",
    "\n",
    "def create_time_vector_arrays(training_data, age_arrays, corpus, ner_vocab, ner_corpus, ner_idf_dict):\n",
    "    len_feature_vector = len(age_arrays[0]) + len(ner_vocab) + 1\n",
    "    vector_array = zeros((len(training_data), len_feature_vector))\n",
    "    results_array = zeros(len(training_data))\n",
    "    index = 0\n",
    "    for data, time in training_data.items():\n",
    "        results_array[index] = time\n",
    "        novel = corpus[data]\n",
    "        age_vector = age_arrays[index]\n",
    "        vector_array[index] = feature_vector_time_period(age_vector, novel, ner_vocab, ner_corpus[data], ner_idf_dict)\n",
    "        index += 1\n",
    "    return (vector_array, results_array)\n",
    "\n",
    "tagset = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "training_data_age = create_age_vector_arrays(training_age, corpus, vocab, idf_dict, tagset)\n",
    "training_data_time = create_time_vector_arrays(training_time, training_data_age[0], corpus, ne_vocab, ne_corpus, ne_idf_dict)\n",
    "testing_data_age = create_age_vector_arrays(testing_age, test_corpus, vocab, test_idf_dict, tagset)\n",
    "testing_data_time = create_time_vector_arrays(testing_time, testing_data_age[0], test_corpus, ne_vocab, test_ne_corpus, test_ne_idf_dict)\n",
    "print('finished feature arrays')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import savetxt\n",
    "savetxt(\"train_features_age.csv\", training_data_age[0], delimiter=\",\")\n",
    "savetxt(\"train_target_age.csv\", training_data_age[1], delimiter=\",\")\n",
    "savetxt(\"test_features_age.csv\", testing_data_age[0], delimiter=\",\")\n",
    "savetxt(\"test_target_age.csv\", testing_data_age[1], delimiter=\",\")\n",
    "\n",
    "savetxt(\"train_features_time.csv\", training_data_time[0], delimiter=\",\")\n",
    "savetxt(\"train_target_time.csv\", training_data_time[1], delimiter=\",\")\n",
    "savetxt(\"test_features_time.csv\", testing_data_time[0], delimiter=\",\")\n",
    "savetxt(\"test_target_time.csv\", testing_data_time[1], delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from math import floor\n",
    "from statistics import mean\n",
    "from numpy import array\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# Function to scale input vectors\n",
    "def scale(scaler, input_vector):\n",
    "    scaler.fit(input_vector)\n",
    "    scaled_features = scaler.transform(input_vector)\n",
    "    return scaled_features\n",
    "# Function to calculate number of nodes in hidden layer of MLP\n",
    "def calc_node_num(input_len, output_len):\n",
    "    return floor(mean([input_len, output_len]))\n",
    "# Function to construct params dictionary to pass to cross validation\n",
    "def construct_params(node_num):\n",
    "    return {'n_estimators': [10, 50, 100], \n",
    "            'max_depth': [None, 5, 10],\n",
    "            'max_features': ['auto', 'log2']}\n",
    "# Function to run 5-fold cross validation (with MLP) to determine optimal hyperparameters for MLP classifer\n",
    "def cross_validation(x_features, y_labels, params):\n",
    "    clf = GridSearchCV(RandomForestClassifier(), params, cv=5,\n",
    "                       scoring='accuracy') # Determine why F1 score was not working earlier\n",
    "    clf.fit(x_features, y_labels)\n",
    "    return clf.best_params_\n",
    "# Return model with given params\n",
    "def get_model(params):\n",
    "    return RandomForestClassifier(n_estimators=params['n_estimators'], \n",
    "                         max_depth=params['max_depth'], \n",
    "                         max_features=params['max_features'])\n",
    "# drop out unimportant features\n",
    "def drop_out(model, x_vector, y_vector):\n",
    "    selected_features_model = SelectFromModel(model)\n",
    "    selected_features_model.fit(x_vector, y_vector)\n",
    "    return selected_features_model\n",
    "\n",
    "# Train model on entire training set\n",
    "def train_model(model, x_vector, y_vector):\n",
    "    model.fit(x_vector, y_vector)\n",
    "    return model\n",
    "# Steps for age and time period classifiers\n",
    "# (0) Scale input/output data (?)\n",
    "scaler = StandardScaler()\n",
    "age_features = training_data_age[0]\n",
    "age_labels = training_data_age[1]\n",
    "scaled_age_features = scale(scaler, age_features)\n",
    "# print(scaled_age_features)\n",
    "time_period_features = training_data_time[0]\n",
    "time_period_labels = training_data_time[1]\n",
    "scaled_time_period_features = scale(scaler, time_period_features)\n",
    "# print(scaled_time_period_features)\n",
    "# (1) Perform cross validation with all chosen parameters\n",
    "age_node_num = calc_node_num(len(age_features[0]), len(age_labels))\n",
    "age_params = construct_params(age_node_num)\n",
    "best_age_params = cross_validation(scaled_age_features, age_labels, age_params)\n",
    "tp_node_num = calc_node_num(len(time_period_features[0]), len(time_period_labels))\n",
    "time_period_params = construct_params(tp_node_num)\n",
    "best_tp_params = cross_validation(scaled_time_period_features, time_period_labels, time_period_params)\n",
    "# (2) After determining optimal params based on accuracy, retrain model with those params\n",
    "age_model = get_model(best_age_params)\n",
    "undropped_age_model = train_model(age_model, scaled_age_features, age_labels)\n",
    "dropped_age_model = drop_out(undropped_age_model, scaled_age_features, age_labels)\n",
    "important_age_features = dropped_age_model.transform(scaled_age_features)\n",
    "trained_age_model = train_model(age_model, important_age_features, age_labels)\n",
    "\n",
    "tp_model = get_model(best_tp_params)\n",
    "undropped_tp_model = train_model(tp_model, scaled_time_period_features, time_period_labels)\n",
    "dropped_tp_model = drop_out(undropped_tp_model, scaled_time_period_features, time_period_labels)\n",
    "important_tp_features = dropped_tp_model.transform(scaled_time_period_features)\n",
    "trained_tp_model = train_model(tp_model, important_tp_features, time_period_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_feature_labels(labels, sf_model):\n",
    "    new_labels = []\n",
    "    indexes = sf_model.get_support(indices=True)\n",
    "    print(indexes)\n",
    "    print(len(labels))\n",
    "    new_labels = [labels[index] for index in indexes]\n",
    "    return new_labels\n",
    "\n",
    "age_feature_names = adjust_feature_labels(age_feature_names, dropped_age_model)\n",
    "time_feature_names = adjust_feature_labels(time_feature_names, dropped_tp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from math import floor\n",
    "# from statistics import mean\n",
    "# from numpy import array\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.neural_network import MLPClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# # Function to scale input vectors\n",
    "# def scale(scaler, input_vector):\n",
    "#     scaler.fit(input_vector)\n",
    "#     scaled_features = scaler.transform(input_vector)\n",
    "#     return scaled_features\n",
    "\n",
    "# # Function to calculate number of nodes in hidden layer of MLP\n",
    "# def calc_node_num(input_len, output_len):\n",
    "#     return floor(mean([input_len, output_len]))\n",
    "\n",
    "# # Function to construct params dictionary to pass to cross validation\n",
    "# def construct_params(node_num):\n",
    "#     return {'hidden_layer_sizes': [(node_num,)], \n",
    "#             'activation': ['logistic', 'tanh', 'relu'],\n",
    "#             'solver': ['lbfgs', 'sgd']}\n",
    "\n",
    "# # Function to run 5-fold cross validation (with MLP) to determine optimal hyperparameters for MLP classifer\n",
    "# def cross_validation(x_features, y_labels, params):\n",
    "#     clf = GridSearchCV(MLPClassifier(), params, cv=5,\n",
    "#                        scoring='accuracy') # Determine why F1 score was not working earlier\n",
    "#     clf.fit(x_features, y_labels)\n",
    "#     return clf.best_params_\n",
    "\n",
    "# # Return model with given params\n",
    "# def get_model(params):\n",
    "#     return MLPClassifier(hidden_layer_sizes=params['hidden_layer_sizes'], \n",
    "#                          activation=params['activation'], \n",
    "#                          solver=params['solver'], max_iter=500)\n",
    "\n",
    "# # Train model on entire training set\n",
    "# def train_model(model, x_vector, y_vector):\n",
    "#     model.fit(x_vector, y_vector)\n",
    "#     return model\n",
    "\n",
    "# # Steps for age and time period classifiers\n",
    "# # (0) Scale input/output data (?)\n",
    "# scaler = StandardScaler()\n",
    "# age_features = training_data_age[0]\n",
    "# age_labels = training_data_age[1]\n",
    "# scaled_age_features = scale(scaler, age_features)\n",
    "# # print(scaled_age_features)\n",
    "\n",
    "# time_period_features = training_data_time[0]\n",
    "# time_period_labels = training_data_time[1]\n",
    "# scaled_time_period_features = scale(scaler, time_period_features)\n",
    "# # print(scaled_time_period_features)\n",
    "\n",
    "# # (1) Perform cross validation with all chosen parameters\n",
    "# age_node_num = calc_node_num(len(age_features[0]), len(age_labels))\n",
    "# age_params = construct_params(age_node_num)\n",
    "# best_age_params = cross_validation(scaled_age_features, age_labels, age_params)\n",
    "\n",
    "# tp_node_num = calc_node_num(len(time_period_features[0]), len(time_period_labels))\n",
    "# time_period_params = construct_params(tp_node_num)\n",
    "# best_tp_params = cross_validation(scaled_time_period_features, time_period_labels, time_period_params)\n",
    "\n",
    "# # (2) After determining optimal params based on accuracy, retrain model with those params\n",
    "# age_model = get_model(best_age_params)\n",
    "# trained_age_model = train_model(age_model, scaled_age_features, age_labels)\n",
    "# tp_model = get_model(best_tp_params)\n",
    "# trained_tp_model = train_model(tp_model, scaled_time_period_features, time_period_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from yellowbrick.classifier import ClassPredictionError, ClassificationReport\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "from numpy import append\n",
    "import matplotlib.pyplot as plt\n",
    "from yellowbrick.style import set_palette\n",
    "    \n",
    "# Palette only needs to be set globally once\n",
    "# set_palette('set1') \n",
    "\n",
    "# Function to produce visualization of class prediction\n",
    "def predict_class_error(model, training_features, training_labels, test_features, test_labels, class_labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "#     ax.set_xlim(auto=True)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=30)\n",
    "    visualizer = ClassPredictionError(model, classes=class_labels, ax=ax)\n",
    "    print(\"Accuracy: %s\" % visualizer.score(test_features, test_labels))\n",
    "    g = visualizer.poof()\n",
    "    \n",
    "# Function to produce visualization of metrics\n",
    "def predict_metrics(model, training_features, training_labels, test_features, test_labels, class_labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    visualizer = ClassificationReport(model, classes=class_labels, support=True, ax=ax)\n",
    "    print(\"Accuracy: %s\" % visualizer.score(test_features, test_labels))\n",
    "    g = visualizer.poof()\n",
    "    \n",
    "# Function to produce visualization of most important features in classifying novel\n",
    "# Note: new model refit with top 10 values of importance from entire model\n",
    "def predict_feature_importance(model, best_params, features, targets, feature_names):\n",
    "    importances = model.feature_importances_\n",
    "    importance_name_dict = {}\n",
    "    feature_index_dict = {}\n",
    "    \n",
    "    # Initialize importance to name mappings and feature name to index mappings\n",
    "    for i in range(0, len(feature_names)):\n",
    "        importance_name_dict[importances[i]] = feature_names[i]\n",
    "        feature_index_dict[feature_names[i]] = i\n",
    "    \n",
    "    # Sort list of importances to find ten with highest importances\n",
    "    highest_importances = (sorted(importances, reverse = True))[0:10]\n",
    "    highest_names = []\n",
    "    highest_indexes = []\n",
    "    \n",
    "    # For every highest importance, find matching name in related map\n",
    "    for importance in highest_importances:\n",
    "        name = importance_name_dict[importance]\n",
    "        highest_names.append(name)\n",
    "        highest_indexes.append(feature_index_dict[name])\n",
    "        \n",
    "    # Remove all features except those with highest importance\n",
    "    new_features = []\n",
    "    for feature in features:\n",
    "        new_feature = [feature[i] for i in highest_indexes]\n",
    "        new_features.append(new_feature)\n",
    "    \n",
    "    # Fit new model with extracted features to highlight their importance\n",
    "    new_model = get_model(best_age_params)\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot()\n",
    "    visualizer = FeatureImportances(new_model, ax=ax, labels=highest_names)\n",
    "    visualizer.fit(new_features, targets)\n",
    "    g = visualizer.poof()\n",
    "    \n",
    "# (3) Use test set to return precision, recall, and f1 scores\n",
    "print(\"AGE CALCULATIONS\")\n",
    "age_classes = [\"18-24\", \"25-34\", \"35-49\", \"50-64\", \"65-x\"]\n",
    "test_age_features = testing_data_age[0]\n",
    "test_age_labels = testing_data_age[1]\n",
    "undropped_test_age_features = scale(scaler, test_age_features)\n",
    "scaled_test_age_features = dropped_age_model.transform(undropped_test_age_features)\n",
    "\n",
    "# Age predictions based on random assignment\n",
    "print(\"RANDOM CALCULATIONS\")\n",
    "dummy_age_classifier = DummyClassifier()\n",
    "trained_dummy_age_classifier = train_model(dummy_age_classifier, scaled_age_features, age_labels)\n",
    "# predict_class_error(trained_dummy_age_classifier, scaled_age_features, age_labels, \n",
    "#                     scaled_test_age_features, test_age_labels, age_classes)\n",
    "predict_metrics(trained_dummy_age_classifier, scaled_age_features, age_labels, \n",
    "                    scaled_test_age_features, test_age_labels, age_classes)\n",
    "\n",
    "# Age Predictions based on trained model\n",
    "print(\"FINAL CALCULATIONS\")\n",
    "# predict_class_error(trained_age_model, scaled_age_features, age_labels, \n",
    "#                     scaled_test_age_features, test_age_labels, age_classes)\n",
    "print(trained_age_model)\n",
    "predict_metrics(trained_age_model, scaled_age_features, age_labels, \n",
    "                    scaled_test_age_features, test_age_labels, age_classes)\n",
    "predict_feature_importance(trained_age_model, best_age_params, scaled_age_features, age_labels, age_feature_names)\n",
    "\n",
    "print(\"TIME PERIOD CALCULATIONS\")\n",
    "tp_classes = [\"1751-1800\", \"1801-1820\", \"1821-1840\", \"1841-1860\", \"1861-1880\", \"1881-1900\"]\n",
    "test_tp_features = testing_data_time[0]\n",
    "test_tp_features = append(test_tp_features, [test_tp_features[0]], axis=0)\n",
    "test_tp_labels = testing_data_time[1]\n",
    "test_tp_labels = append(test_tp_labels, 0)\n",
    "undropped_test_tp_features = scale(scaler, test_tp_features)\n",
    "scaled_test_tp_features = dropped_tp_model.transform(undropped_test_tp_features)\n",
    "\n",
    "print(\"RANDOM CALCULATIONS\")\n",
    "dummy_tp_classifier = DummyClassifier()\n",
    "trained_dummy_tp_classifier = train_model(dummy_tp_classifier, scaled_time_period_features, time_period_labels)\n",
    "# predict_class_error(trained_dummy_tp_classifier, scaled_time_period_features, time_period_labels, \n",
    "#                    scaled_test_tp_features, test_tp_labels, tp_classes)\n",
    "predict_metrics(trained_dummy_tp_classifier, scaled_time_period_features, time_period_labels, \n",
    "                    scaled_test_tp_features, test_tp_labels, tp_classes)\n",
    "\n",
    "print(\"FINAL CALCULATIONS\")\n",
    "# predict_class_error(trained_tp_model, scaled_time_period_features, time_period_labels, \n",
    "#                    scaled_test_tp_features, test_tp_labels, tp_classes)\n",
    "predict_metrics(trained_tp_model, scaled_time_period_features, time_period_labels, \n",
    "                    scaled_test_tp_features, test_tp_labels, tp_classes)\n",
    "predict_feature_importance(trained_tp_model, best_tp_params, scaled_time_period_features, \n",
    "                           time_period_labels, time_feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

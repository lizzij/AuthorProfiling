{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import nltk\n",
    "import re\n",
    "import operator\n",
    "import math\n",
    "import readability\n",
    "import random\n",
    "from nltk import word_tokenize\n",
    "from gensim.models import Word2Vec\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from numpy import array\n",
    "from numpy import mean\n",
    "from numpy import zeros\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "nltk.internals.config_java(options='-Xmx3024m')\n",
    "st = StanfordPOSTagger('english-left3words-distsim.tagger', path_to_jar='stanford-postagger.jar')\n",
    "ner = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz', 'stanford-ner.jar')\n",
    "\n",
    "def train_test_split(directory):\n",
    "    file_names = os.listdir(directory)\n",
    "    random.shuffle(file_names)\n",
    "    train_list = []\n",
    "    test_list = []\n",
    "    i = 0\n",
    "    for file_name in file_names:\n",
    "        if file_name != '.DS_Store':\n",
    "            if i < 4:\n",
    "                train_list.append(file_name)\n",
    "                i+= 1\n",
    "            else:\n",
    "                test_list.append(file_name)\n",
    "                i = 0\n",
    "    return (train_list, test_list)\n",
    "\n",
    "def make_training_dicts(filepath, file_list):\n",
    "    age_dir = {}\n",
    "    time_dir = {}\n",
    "    with open(filepath, encoding='latin-1') as file:\n",
    "        reader = csv.reader(file)\n",
    "        for row in reader:\n",
    "            if row[0] in file_list:\n",
    "                age_dir[row[0]] = row[1]\n",
    "                time_dir[row[0]] = row[2]\n",
    "    return (age_dir, time_dir)\n",
    "\n",
    "def clean_and_tokenize_file(file_name):\n",
    "    file = open(file_name)\n",
    "    raw_text = file.read()\n",
    "    raw_text_lower = raw_text.lower()\n",
    "    caseless_tokens = nltk.word_tokenize(raw_text_lower)\n",
    "    caseless_tokens = [x for x in caseless_tokens if re.match('\\w+', x)]\n",
    "    sentences = nltk.sent_tokenize(raw_text_lower)\n",
    "    raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "    case_tokens = nltk.word_tokenize(raw_text)\n",
    "    sentence_list = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        sentence_list.append([x for x in words if re.match('\\w+', x)])\n",
    "    return (caseless_tokens, sentence_list, raw_sentence_tokens, case_tokens)\n",
    "\n",
    "\n",
    "def clean_and_tokenize_corpus(directory, file_list):\n",
    "    file_dir = {}\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name in file_list:\n",
    "            file_dir[file_name] = clean_and_tokenize_file(directory + file_name)\n",
    "    return file_dir\n",
    "\n",
    "\n",
    "def count_total_vocab(file_wordfreqs):\n",
    "    vocab = {}\n",
    "    for words in file_wordfreqs:\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def get_corpus_vocab(corpus, unknown_threshold):\n",
    "    initial_dict = count_total_vocab(corpus)\n",
    "    sorted_dict = sorted(initial_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    filtered_dict = [key for key, value in sorted_dict[:unknown_threshold]]\n",
    "    return filtered_dict\n",
    "\n",
    "def get_idf_dict(vocab, corpus):\n",
    "    idf_dict = {}\n",
    "    num_docs = len(corpus) \n",
    "    for word in vocab:\n",
    "        df_count = 1\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                df_count += 1\n",
    "        idf_dict[word] = math.log(num_docs / df_count)\n",
    "    return idf_dict\n",
    "\n",
    "def get_named_entities(novel):\n",
    "    named_entities = ner.tag(novel)\n",
    "    entities = []\n",
    "    cur_entity = []\n",
    "    last_tag = 'O'\n",
    "    for entity in named_entities:\n",
    "        if entity[1] == 'O':\n",
    "            if len(cur_entity) > 0:\n",
    "                new_entry = (' '.join(cur_entity), last_tag)\n",
    "                if new_entry not in entities:\n",
    "                    entities.append(new_entry)\n",
    "                cur_entity = []\n",
    "            last_tag = 'O'\n",
    "        elif entity[1] == last_tag:\n",
    "            cur_entity.append(entity[0])\n",
    "        else:\n",
    "            if len(cur_entity) > 0:\n",
    "                new_entry = (' '.join(cur_entity), last_tag)\n",
    "                if new_entry not in entities:\n",
    "                    entities.append(new_entry)\n",
    "                cur_entity = []\n",
    "            cur_entity.append(entity[0])\n",
    "            last_tag = entity[1]\n",
    "    return entities\n",
    "\n",
    "def get_ne_corpus(corpus):\n",
    "    ne_dict = {}\n",
    "    for key, value in corpus.items():\n",
    "        ne_dict[key] = get_named_entities(value[3])\n",
    "    return ne_dict\n",
    "\n",
    "full_list = train_test_split('backup_plan')\n",
    "train_list = full_list[0]\n",
    "test_list = full_list[1]\n",
    "training_data = make_training_dicts('backup.csv', train_list)\n",
    "training_age = training_data[0]\n",
    "training_time = training_data[1]\n",
    "corpus = clean_and_tokenize_corpus('backup_plan/', train_list)\n",
    "ugram_corpus = [value[0] for key, value in corpus.items()]\n",
    "vocab = get_corpus_vocab(ugram_corpus, 500)\n",
    "idf_dict = get_idf_dict(vocab, ugram_corpus)\n",
    "ne_corpus = get_ne_corpus(corpus)\n",
    "ne_vals = [value for key, value in ne_corpus.items()]\n",
    "ne_vocab = get_corpus_vocab(ne_vals, 100)\n",
    "ne_idf_dict = get_idf_dict(ne_vocab, ne_vals)\n",
    "\n",
    "\n",
    "testing_data = make_training_dicts('backup.csv', test_list)\n",
    "testing_age = testing_data[0]\n",
    "testing_time = testing_data[1]\n",
    "test_corpus = clean_and_tokenize_corpus('backup_plan/', test_list)\n",
    "test_ugram_corpus = [value[0] for key, value in test_corpus.items()]\n",
    "test_idf_dict = get_idf_dict(vocab, test_ugram_corpus)\n",
    "test_ne_corpus = get_ne_corpus(test_corpus)\n",
    "test_ne_vals = [value for key, value in test_ne_corpus.items()]\n",
    "test_ne_idf_dict = get_idf_dict(ne_vocab, test_ne_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(corpus):\n",
    "    model = Word2Vec(corpus, size=100)\n",
    "    model.train(corpus, total_examples=len(corpus), epochs=50)\n",
    "    return model\n",
    "\n",
    "word2vec = train_word2vec(ugram_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:43: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "st = StanfordPOSTagger('english-left3words-distsim.tagger', path_to_jar='stanford-postagger.jar')\n",
    "st.java_options = '-mx4g'\n",
    "def clean_and_tokenize_file(file_name):\n",
    "    file = open(file_name)\n",
    "    raw_text = file.read()\n",
    "    raw_text_lower = raw_text.lower()\n",
    "    caseless_tokens = nltk.word_tokenize(raw_text_lower)\n",
    "    sentences = nltk.sent_tokenize(raw_text_lower)\n",
    "    raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "    case_tokens = nltk.word_tokenize(raw_text)\n",
    "    sentence_list = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        sentence_list.append([x for x in words if re.match('\\w+', x)])\n",
    "    return (caseless_tokens, sentence_list, raw_sentence_tokens, case_tokens)\n",
    "\n",
    "def tf_idf(vocab, document, idf_dict):\n",
    "    counts = Counter(document)\n",
    "    doc_len = len(document)\n",
    "    tf_idfs = []\n",
    "    for word in vocab:\n",
    "        doc_counter = 0\n",
    "        tf = counts[word] / len(document)\n",
    "        idf = idf_dict[word]\n",
    "        tf_idfs.append(tf*idf)\n",
    "    return tf_idfs\n",
    "\n",
    "def get_avg_word2vec(novel):\n",
    "    vec_list = []\n",
    "    for word in novel:\n",
    "        if word in word2vec:\n",
    "            vec_list.append(word2vec[word])\n",
    "    vector_array = array(vec_list)\n",
    "    vector_array = mean(vector_array, axis=0)\n",
    "    return vector_array.tolist()\n",
    "\n",
    "def get_pairwise_similarity(sentences):\n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        vec_list = []\n",
    "        for word in sentence:\n",
    "            if word in word2vec:\n",
    "                vec_list.append(word2vec[word])\n",
    "        if len(vec_list) > 0:\n",
    "            vector_array = array(vec_list)\n",
    "            vector_array = mean(vector_array, axis=0)\n",
    "            sentence_vectors.append(vector_array)\n",
    "    cs_sim = cosine_similarity(sentence_vectors, sentence_vectors)\n",
    "    return mean(cs_sim)\n",
    "\n",
    "def get_pos_vector(novel, pos_tags):\n",
    "    tagged_tokens = st.tag(novel)\n",
    "    tags = [tag for (word, tag) in tagged_tokens]\n",
    "    num_tokens = len(novel)\n",
    "    pos_vector = [tags.count(pos_tag) / num_tokens for pos_tag in pos_tags]\n",
    "    return pos_vector\n",
    "\n",
    "def get_readability_score(sentences):\n",
    "    newline_regex = re.compile(r'\\n')\n",
    "    clean_sents = []\n",
    "    for sentence in sentences:\n",
    "        clean_sents.append(re.sub(newline_regex, ' ', sentence))\n",
    "    readability_text = '\\n'.join(clean_sents)\n",
    "    readability_results = readability.getmeasures(readability_text, lang='en')\n",
    "    flesch_score = readability_results['readability grades']['FleschReadingEase']   \n",
    "    return flesch_score\n",
    "\n",
    "def feature_vector(novel, vocab, idf_dict, pos_tags):\n",
    "    tokens = novel[0] # for unigrams\n",
    "    token_sents = novel[1] #  for sent similarity\n",
    "    case_tokens = novel[3] # for pos tagging and ner\n",
    "    \n",
    "    unigram_vector = tf_idf(vocab, tokens, idf_dict)\n",
    "    pos_vector = get_pos_vector(case_tokens, pos_tags)\n",
    "    word_2_vec_vector = get_avg_word2vec(tokens)\n",
    "    word_vector = unigram_vector + pos_vector + word_2_vec_vector\n",
    "    \n",
    "    # gets ratio of unique words to total # of words\n",
    "    novel_vocab = count_total_vocab([tokens])\n",
    "    vocab_size = len(novel_vocab) / len(tokens)\n",
    "    word_vector.append(vocab_size)\n",
    "      \n",
    "    cs_similarity = get_pairwise_similarity(token_sents)\n",
    "    word_vector.append(cs_similarity)\n",
    "    return word_vector\n",
    "\n",
    "def feature_vector_time_period(age_vector, novel, ne_vocab, ne_novel, ne_idf_dict):\n",
    "    raw_sents = novel[2]\n",
    "    ne_vector = tf_idf(ne_vocab, ne_novel, ne_idf_dict)\n",
    "    time_period_vector = age_vector.tolist() + ne_vector\n",
    "    time_period_vector.append(get_readability_score(raw_sents))\n",
    "    return time_period_vector\n",
    "\n",
    "def create_age_vector_arrays(training_data, corpus, vocab, idf_dict, pos_tags):\n",
    "    len_feature_vector = len(vocab) + len(pos_tags) + 102\n",
    "    vector_array = zeros((len(training_data), len_feature_vector))\n",
    "    results_array = zeros(len(training_data))\n",
    "    index = 0\n",
    "    for data, age in training_data.items():\n",
    "        results_array[index] = age\n",
    "        novel = corpus[data]\n",
    "        vector_array[index] = feature_vector(novel, vocab, idf_dict, pos_tags)\n",
    "        index += 1\n",
    "    return (vector_array, results_array)\n",
    "\n",
    "def create_time_vector_arrays(training_data, age_arrays, corpus, ner_vocab, ner_corpus, ner_idf_dict):\n",
    "    len_feature_vector = len(age_arrays[0]) + len(ner_vocab) + 1\n",
    "    vector_array = zeros((len(training_data), len_feature_vector))\n",
    "    results_array = zeros(len(training_data))\n",
    "    index = 0\n",
    "    for data, time in training_data.items():\n",
    "        results_array[index] = time\n",
    "        novel = corpus[data]\n",
    "        age_vector = age_arrays[index]\n",
    "        vector_array[index] = feature_vector_time_period(age_vector, novel, ner_vocab, ner_corpus[data], ner_idf_dict)\n",
    "        index += 1\n",
    "    return (vector_array, results_array)\n",
    "       \n",
    "tagset = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "training_data_age = create_age_vector_arrays(training_age, corpus, vocab, idf_dict, tagset)\n",
    "training_data_time = create_time_vector_arrays(training_time, training_data_age[0], corpus, ne_vocab, ne_corpus, ne_idf_dict)\n",
    "testing_data_age = create_age_vector_arrays(testing_age, test_corpus, vocab, test_idf_dict, tagset)\n",
    "testing_data_time = create_time_vector_arrays(testing_time, testing_data_age[0], test_corpus, ne_vocab, test_ne_corpus, test_ne_idf_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.35475655e-02, -8.29241177e-03, -6.81639734e-03, -6.64981303e-03,\n",
       "       -4.35540478e-03, -3.01885628e-03, -3.59318473e-03, -2.18399773e-03,\n",
       "       -2.53072553e-03, -2.90554021e-03, -2.55203282e-03, -1.90700289e-03,\n",
       "       -2.16656449e-03, -2.09005193e-03, -2.17043854e-03, -1.36754093e-03,\n",
       "       -1.65325238e-03, -1.79174980e-03, -1.43049430e-03, -1.60773225e-03,\n",
       "       -2.10942219e-03, -1.50603834e-03, -1.34235958e-03, -1.38981674e-03,\n",
       "       -1.42952578e-03, -1.25035080e-03, -1.31233566e-03, -1.25906743e-03,\n",
       "       -8.60039903e-04, -7.70936670e-04, -1.09345163e-03, -1.02565570e-03,\n",
       "       -1.50991240e-03, -9.89820699e-04, -9.99505833e-04, -1.12444406e-03,\n",
       "       -8.76504631e-04, -7.26385053e-04, -6.63431682e-04, -6.37281820e-04,\n",
       "       -6.48903981e-04, -9.21056247e-04, -6.49872494e-04, -7.65125589e-04,\n",
       "       -6.01446824e-04, -6.45998441e-04, -6.64400195e-04, -5.77233989e-04,\n",
       "       -5.61737774e-04,  3.80873241e-03, -5.56895207e-04, -5.57863721e-04,\n",
       "       -4.36799545e-04, -4.07744143e-04, -5.55926694e-04, -4.28082925e-04,\n",
       "       -4.06775630e-04, -5.23965752e-04, -4.54232787e-04, -3.87405362e-04,\n",
       "       -5.20091698e-04, -4.35831032e-04, -3.99027523e-04, -4.24208871e-04,\n",
       "       -4.86193729e-04, -3.97090496e-04, -3.78688741e-04, -3.16703883e-04,\n",
       "       -2.19852543e-04, -3.39948205e-04, -4.34862519e-04, -3.07987263e-04,\n",
       "       -3.62224013e-04, -3.65129553e-04,  2.14805288e-03, -2.79900374e-04,\n",
       "       -3.12829830e-04, -3.21546450e-04, -3.07018749e-04, -3.70940634e-04,\n",
       "        2.00665444e-03, -3.13798343e-04, -2.59561592e-04, -2.41159838e-04,\n",
       "       -3.36074151e-04, -3.42853745e-04, -2.91522535e-04, -1.88860114e-04,\n",
       "       -2.32443217e-04, -3.23483477e-04, -2.85711454e-04, -3.07018749e-04,\n",
       "        1.72385756e-03, -2.26632137e-04, -2.76994834e-04, -2.61498619e-04,\n",
       "       -3.60286986e-04, -3.91279415e-04, -2.45033891e-04, -2.03387815e-04,\n",
       "       -1.78206466e-04, -2.06293355e-04, -2.54719025e-04, -1.66584306e-04,\n",
       "        1.24863094e-06, -2.50844972e-04, -2.66341186e-04, -1.74332413e-04,\n",
       "       -3.06050236e-04, -2.00482275e-04, -2.46002405e-04, -3.97090496e-04,\n",
       "       -2.09198895e-04, -2.23726596e-04, -1.81112007e-04, -1.84017547e-04,\n",
       "       -1.67552819e-04, -1.16221609e-04, -2.78931860e-04, -2.55687539e-04,\n",
       "       -1.50119578e-04, -1.63678765e-04, -2.41159838e-04, -2.11135922e-04,\n",
       "       -2.30506190e-04, -1.65615792e-04, -1.70458359e-04, -1.80143493e-04,\n",
       "       -2.14041462e-04, -2.07261869e-04, -1.62710252e-04, -1.31717823e-04,\n",
       "       -1.83049033e-04, -1.84986060e-04, -3.56412933e-04, -1.92734167e-04,\n",
       "       -1.94671194e-04, -2.29537677e-04, -1.71426873e-04, -1.21064176e-04,\n",
       "       -1.77237953e-04, -3.11861316e-04, -1.82080520e-04, -9.10402600e-05,\n",
       "       -1.35591877e-04, -1.53993631e-04, -1.32686336e-04, -1.27843769e-04,\n",
       "       -1.80143493e-04,  1.04093319e-03, -1.73363899e-04, -1.24938229e-04,\n",
       "       -1.97576734e-04, -9.97568806e-05, -1.52056604e-04, -9.87883672e-05,\n",
       "       -2.71183753e-04, -1.56899171e-04, -7.07014785e-05, -2.00482275e-04,\n",
       "        0.00000000e+00, -1.14284582e-04, -1.03630934e-04, -1.29780796e-04,\n",
       "       -1.34623363e-04, -1.35591877e-04, -8.61976930e-05, -1.03630934e-04,\n",
       "       -8.52291796e-05, -1.51088091e-04,  0.00000000e+00, -6.68274249e-05,\n",
       "       -1.17190122e-04, -1.04599448e-04, -1.01693907e-04,  8.33348245e-04,\n",
       "       -1.54962145e-04, -1.47214037e-04, -1.25906743e-04, -1.67552819e-04,\n",
       "       -8.32921528e-05, -6.87644517e-05,  7.91229561e-04, -3.00239155e-05,\n",
       "        0.00000000e+00, -1.11379041e-04, -1.23001202e-04, -1.51088091e-04,\n",
       "       -9.00717466e-05, -1.33654850e-04, -8.52291796e-05, -9.78198538e-05,\n",
       "       -1.47214037e-04, -1.43339984e-04, -7.74810723e-05, -1.37528903e-04,\n",
       "       -6.77959383e-05, -2.22758083e-05, -7.36070187e-05, -8.32921528e-05,\n",
       "       -9.29772868e-05, -1.20095662e-04, -1.35591877e-04, -5.52052640e-05,\n",
       "       -1.03630934e-04, -1.95639708e-04, -6.77959383e-06, -1.04599448e-04,\n",
       "       -1.09442015e-04, -6.19848579e-05,  0.00000000e+00, -1.10410528e-04,\n",
       "       -7.45755321e-05, -9.29772868e-05,  0.00000000e+00, -1.34623363e-04,\n",
       "       -9.68513404e-05, -4.93941836e-05, -8.61976930e-05, -9.49143136e-05,\n",
       "       -5.90793177e-05, -1.00725394e-04, -1.10410528e-04, -1.26875256e-04,\n",
       "       -1.16221609e-04, -1.93702681e-04, -1.84017547e-04, -9.00717466e-05,\n",
       "       -7.84495857e-05, -9.49143136e-05, -8.81347198e-05, -1.74332413e-05,\n",
       "       -8.03866126e-05, -8.61976930e-05, -1.16221609e-04, -9.10402600e-05,\n",
       "       -3.09924289e-05, -1.37528903e-04, -5.90793177e-05, -6.39218847e-05,\n",
       "       -6.48903981e-05, -8.13551260e-05, -6.77959383e-05, -4.45516166e-05,\n",
       "       -1.21064176e-04, -8.61976930e-05, -1.10410528e-04, -1.11379041e-04,\n",
       "       -8.32921528e-05, -6.77959383e-05, -8.71662064e-05, -4.93941836e-05,\n",
       "       -7.74810723e-05, -7.16699919e-05, -6.48903981e-05, -1.15253095e-04,\n",
       "       -8.61976930e-05, -8.71662064e-05, -8.52291796e-05, -8.13551260e-05,\n",
       "       -7.94180991e-05, -5.61737774e-05, -6.77959383e-05, -9.97568806e-05,\n",
       "       -4.45516166e-05, -8.61976930e-05, -1.27843769e-04, -8.13551260e-05,\n",
       "       -7.36070187e-05, -8.42606662e-05, -8.91032332e-05, -6.39218847e-05,\n",
       "       -9.68513404e-05, -8.03866126e-05, -6.77959383e-05, -8.81347198e-05,\n",
       "       -1.21064176e-04, -1.36560390e-04, -5.81108043e-05, -6.77959383e-05,\n",
       "       -5.03626970e-05, -5.22997238e-05, -3.58349960e-05, -9.00717466e-05,\n",
       "       -8.13551260e-05,  0.00000000e+00, -8.81347198e-05,  1.99780951e-04,\n",
       "        4.87373342e-04, -8.23236394e-05, -4.93941836e-05,  4.84364865e-04,\n",
       "       -6.10163445e-05, -4.06775630e-05, -6.29533713e-05, -2.90554021e-06,\n",
       "       -7.26385053e-05, -5.90793177e-05, -7.26385053e-05, -9.00717466e-05,\n",
       "       -1.03630934e-04, -6.58589115e-05, -7.55440455e-05, -7.36070187e-05,\n",
       "        2.49726189e-06, -7.84495857e-05, -7.84495857e-05,  0.00000000e+00,\n",
       "       -7.26385053e-05, -4.26145898e-05, -5.61737774e-05, -4.26145898e-05,\n",
       "       -5.03626970e-05, -8.03866126e-05, -8.13551260e-05, -8.61976930e-05,\n",
       "       -5.22997238e-05, -5.22997238e-05, -8.71662064e-05, -8.91032332e-05,\n",
       "       -3.58349960e-05, -1.25906743e-04, -5.52052640e-05, -7.55440455e-05,\n",
       "       -7.55440455e-05, -7.45755321e-05, -3.58349960e-05,  0.00000000e+00,\n",
       "       -7.94180991e-05, -3.97090496e-05,  0.00000000e+00, -8.23236394e-05,\n",
       "        0.00000000e+00, -3.29294557e-05, -9.10402600e-05, -7.07014785e-05,\n",
       "       -2.42128351e-05,  1.66067916e-04, -5.90793177e-05, -2.32443217e-05,\n",
       "       -4.35831032e-05, -3.58349960e-05, -5.32682372e-05, -5.61737774e-05,\n",
       "       -1.19127149e-04, -9.97568806e-05,  0.00000000e+00, -6.48903981e-05,\n",
       "       -6.29533713e-05, -3.48664826e-05, -8.32921528e-05, -1.04599448e-04,\n",
       "       -8.32921528e-05, -6.77959383e-05, -4.64886434e-05, -7.84495857e-05,\n",
       "       -6.48903981e-05, -4.35831032e-05, -5.90793177e-05,  0.00000000e+00,\n",
       "       -5.32682372e-05,  0.00000000e+00, -7.55440455e-05, -6.48903981e-05,\n",
       "       -6.39218847e-05, -7.07014785e-05, -9.97568806e-05, -8.52291796e-05,\n",
       "       -4.74571568e-05, -4.55201300e-05, -8.52291796e-05, -6.87644517e-05,\n",
       "       -6.00478311e-05, -4.84256702e-05, -3.29294557e-05, -7.94180991e-05,\n",
       "       -7.16699919e-05, -6.68274249e-05, -4.84256702e-05, -8.32921528e-05,\n",
       "       -7.94180991e-05,  1.57327499e-04, -2.32443217e-05, -7.07014785e-05,\n",
       "       -4.26145898e-05, -8.03866126e-05, -7.16699919e-05, -8.32921528e-05,\n",
       "       -2.90554021e-05, -3.48664826e-05, -4.74571568e-05, -4.06775630e-05,\n",
       "       -9.00717466e-05, -6.00478311e-05, -3.77720228e-05,  0.00000000e+00,\n",
       "       -3.19609423e-05, -7.45755321e-05, -3.29294557e-05, -2.71183753e-05,\n",
       "       -5.42367506e-05, -5.81108043e-05, -6.58589115e-05, -2.90554021e-05,\n",
       "       -1.06536474e-05, -5.61737774e-05, -7.74810723e-05, -5.71422909e-05,\n",
       "       -5.22997238e-05, -5.90793177e-05,  0.00000000e+00, -4.84256702e-05,\n",
       "       -2.61498619e-05,  3.51991858e-04,  0.00000000e+00, -4.06775630e-05,\n",
       "       -7.84495857e-05, -7.07014785e-05, -1.84017547e-05, -3.38979691e-05,\n",
       "       -8.61976930e-05, -6.68274249e-05, -7.26385053e-05, -6.10163445e-05,\n",
       "       -1.45277011e-05, -6.10163445e-05, -3.00239155e-05, -7.45755321e-05,\n",
       "       -4.45516166e-05, -6.58589115e-05, -6.68274249e-05, -5.71422909e-05,\n",
       "       -6.10163445e-05, -2.13072949e-05, -4.45516166e-05, -4.64886434e-05,\n",
       "       -5.13312104e-05, -3.87405362e-05, -3.68035094e-05, -5.71422909e-05,\n",
       "        0.00000000e+00, -2.13072949e-05, -3.68035094e-05, -3.77720228e-05,\n",
       "       -2.32443217e-05, -4.06775630e-05, -3.87405362e-05,  0.00000000e+00,\n",
       "       -6.19848579e-05, -4.93941836e-05, -4.45516166e-05, -4.93941836e-05,\n",
       "       -5.03626970e-05, -2.61498619e-05, -2.32443217e-05,  3.21907084e-04,\n",
       "       -3.68035094e-05, -5.71422909e-05, -2.80868887e-05,  3.18898606e-04,\n",
       "       -4.16460764e-05, -5.61737774e-05, -7.26385053e-05,  1.26111725e-04,\n",
       "        0.00000000e+00, -8.03866126e-05, -2.71183753e-05,  1.12376785e-04,\n",
       "       -3.00239155e-05, -5.52052640e-05, -6.29533713e-05, -7.07014785e-05,\n",
       "       -7.16699919e-05,  0.00000000e+00, -3.68035094e-05, -8.52291796e-05,\n",
       "       -6.48903981e-05, -3.87405362e-05, -6.77959383e-05, -7.45755321e-05,\n",
       "       -1.06536474e-05, -3.87405362e-05, -5.03626970e-05, -4.16460764e-05,\n",
       "       -6.87644517e-05, -5.61737774e-05, -3.38979691e-05, -4.55201300e-05,\n",
       "        1.19868571e-04, -2.61498619e-05, -4.93941836e-05, -2.42128351e-05,\n",
       "       -4.74571568e-05, -4.93941836e-05, -4.35831032e-05, -1.74332413e-05,\n",
       "        0.00000000e+00, -3.38979691e-05, -5.61737774e-05, -2.03387815e-05,\n",
       "        3.57452103e-02,  1.11619649e-02,  8.63415096e-02,  1.35340644e-03,\n",
       "        2.05921517e-03,  1.13249558e-01,  5.98081947e-02,  2.35754670e-03,\n",
       "        1.69539624e-03,  0.00000000e+00,  1.42689786e-02,  1.29646877e-01,\n",
       "        2.96075849e-02,  4.05257911e-02,  1.06962767e-03,  1.09145680e-03,\n",
       "        0.00000000e+00,  5.76361956e-02,  2.98367908e-02,  4.15408460e-02,\n",
       "        1.83364743e-03,  8.36783550e-04,  2.14289352e-03,  2.49834462e-02,\n",
       "        6.03939431e-04,  3.52686075e-02,  4.66233965e-02,  1.14966783e-02,\n",
       "        2.56092148e-02,  1.64264249e-02,  1.14530201e-02,  9.51750333e-03,\n",
       "        6.26496205e-03,  6.29406757e-04,  4.38401816e-03, -9.73272175e-02,\n",
       "        3.58552933e-01,  1.33743938e-02, -6.18761256e-02,  1.50659367e-01,\n",
       "        1.15960568e-01, -2.53878593e-01,  4.96605001e-02, -1.32742479e-01,\n",
       "       -5.68588376e-02, -2.37905696e-01, -1.25111207e-01,  5.61448783e-02,\n",
       "       -7.47331977e-02, -1.68858588e-01, -5.51056862e-01, -8.98640323e-03,\n",
       "       -5.09184301e-01,  7.44915381e-02, -2.41172418e-01, -1.46527275e-01,\n",
       "        2.66561896e-01, -8.81977320e-01,  2.07706183e-01, -1.08237572e-01,\n",
       "        3.62063497e-02,  3.08750570e-01, -1.87691584e-01,  1.43482730e-01,\n",
       "       -1.92990303e-01, -8.81672427e-02, -6.53364584e-02,  3.59493822e-01,\n",
       "        1.33553728e-01,  3.06048304e-01,  2.18576074e-01,  2.01882035e-01,\n",
       "       -2.64966562e-02,  3.67786914e-01,  2.41569921e-01, -1.41827032e-01,\n",
       "        2.75097132e-01,  1.68212280e-01,  6.21192753e-01, -3.02329957e-01,\n",
       "        5.02261072e-02, -9.32856202e-02,  7.76172951e-02, -6.97228387e-02,\n",
       "        1.46108374e-01,  3.40469509e-01, -5.46699941e-01,  3.93885344e-01,\n",
       "        1.78079307e-01, -1.59460217e-01,  1.84345916e-01, -4.25842293e-02,\n",
       "       -9.78988707e-02, -1.86411038e-01, -2.94310391e-01, -3.07517588e-01,\n",
       "       -3.19962233e-01,  6.45875782e-02,  2.44050205e-01, -6.31755143e-02,\n",
       "        1.27767995e-01, -3.10020179e-01, -7.83137828e-02,  1.61669612e-01,\n",
       "        5.07101715e-01,  1.49453476e-01, -1.88822567e-01,  4.19544317e-02,\n",
       "        4.47731987e-02, -2.88559318e-01, -3.08766574e-01,  6.69391528e-02,\n",
       "       -5.55886291e-02,  8.02139640e-02, -2.45241135e-01,  4.30858940e-01,\n",
       "        1.33431911e-01, -1.51024431e-01,  3.18389088e-01, -1.39241159e-01,\n",
       "       -4.71250564e-01, -2.48805717e-01,  2.91005105e-01, -1.19945385e-01,\n",
       "       -3.19598794e-01, -1.80159777e-01, -4.65539813e-01, -8.67958516e-02,\n",
       "       -1.81531847e-01,  2.97502905e-01, -5.62894940e-01, -5.38611747e-02,\n",
       "        1.71795473e-01,  5.05521476e-01, -2.34750718e-01,  6.95101520e-02,\n",
       "        6.68325126e-01, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00, -0.00000000e+00, -0.00000000e+00,\n",
       "       -0.00000000e+00, -0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  4.77717794e+01])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_array = array(features_vector_array[0], dtype='float32')\n",
    "input_vector = from_numpy(input_array)\n",
    "\n",
    "output_array = array(features_vector_array[1], dtype='float32')\n",
    "target_vector = from_numpy(output_array)\n",
    "\n",
    "input_and_target = TensorDataset(input_vector, target_vector)\n",
    "\n",
    "predict_array = array(test_vector, dtype='float32')\n",
    "predict_vector = from_numpy(predict_array)\n",
    "\n",
    "dickens_predict_array = array(dickens_test_array, dtype='float32')\n",
    "dickens_predict_vector = from_numpy(dickens_predict_array)\n",
    "\n",
    "linear_model = nn.Linear(115, 1)\n",
    "loss_func = nn.functional.mse_loss \n",
    "optimize = optim.SGD(linear_model.parameters(), lr=0.001)\n",
    "loss = loss_func(linear_model(input_vector), target_vector)\n",
    "\n",
    "def train(num_epochs, model, loss_func, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in input_and_target:\n",
    "            \n",
    "            # Predictions\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            \n",
    "            # Stochastic radient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    print(loss)\n",
    "\n",
    "train(500, linear_model, loss_func, optimize)\n",
    "\n",
    "pred = linear_model(predict_vector)\n",
    "print(pred)\n",
    "\n",
    "def goodness(predicted, actual, range_size):\n",
    "    return float((predicted - actual)) / float(range_size)\n",
    "\n",
    "print(\"Jane Austen\")\n",
    "print(\"Age predicted for Mansfield Park: %s\" % pred)\n",
    "print(\"Goodness Metric: %s\\n\" % abs(goodness(pred, 39, 15)))\n",
    "\n",
    "pred = linear_model(dickens_predict_vector)\n",
    "print(\"Charles Dickens\")\n",
    "print(\"Age predicted for Oliver Twist: %s\" % pred)\n",
    "print(\"Goodness Metric: %s\" % abs(goodness(pred, 25, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

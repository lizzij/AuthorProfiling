{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[1.00000000e+00, 1.00000000e+00, 1.00000000e+00, ...,\n",
       "         3.75934976e-04, 4.60196264e-03, 1.45499799e-01],\n",
       "        [0.00000000e+00, 0.00000000e+00, 1.00000000e+00, ...,\n",
       "         2.41360256e-04, 4.56043851e-03, 1.10822467e-01]]), array([28., 40.]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import numpy\n",
    "from nltk import word_tokenize\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "def clean_and_tokenize_file(file_name):\n",
    "    file = open(file_name)\n",
    "    raw_text= file.read()\n",
    "    words = raw_text.split()\n",
    "    return words\n",
    "\n",
    "def clean_and_tokenize_corpus(directories):\n",
    "    file_list = []\n",
    "    for directory in directories:\n",
    "        for file_name in os.listdir(directory):\n",
    "            if file_name != '.DS_Store':\n",
    "                file = open(file_name)\n",
    "                raw_text = file.read()\n",
    "                tokens = raw_text.split()\n",
    "                file_list.append(tokens)\n",
    "                file.close()\n",
    "    return file_list\n",
    "\n",
    "def count_total_vocab(file_wordfreqs):\n",
    "    vocab = {}\n",
    "    for words in file_wordfreqs:\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def get_corpus_vocab(corpus, unknown_threshold):\n",
    "    initial_dict = count_total_vocab(corpus)\n",
    "    filtered_dict = [word for word, size in initial_dict.items()][:unknown_threshold]\n",
    "    return filtered_dict\n",
    "\n",
    "def feature_vector(novel, vocab, corpus, pos_tags):\n",
    "    tokenized_novel = clean_and_tokenize_file(novel)\n",
    "    word_vector = tf_idf(vocab, tokenized_novel, corpus)\n",
    "    tagged_tokens = nltk.pos_tag(tokenized_novel)\n",
    "    tags = [tag for (word, tag) in tagged_tokens]\n",
    "    num_tokens = len(tokenized_novel)\n",
    "    pos_vector = [tags.count(pos_tag) / num_tokens for pos_tag in pos_tags]\n",
    "    word_vector = word_vector + pos_vector\n",
    "    \n",
    "    # gets ratio of unique words to total # of words\n",
    "    novel_vocab = count_total_vocab([tokenized_novel])\n",
    "    vocab_size = len(novel_vocab) / len(tokenized_novel)\n",
    "    word_vector.append(vocab_size)\n",
    "    \n",
    "    return word_vector\n",
    "\n",
    "def tf_idf(vocab, document, corpus):\n",
    "    counts = Counter(document)\n",
    "    doc_len = len(document)\n",
    "    doc_num = len(corpus)\n",
    "    tf_idfs = []\n",
    "    for word in vocab:\n",
    "        doc_counter = 0\n",
    "        tf = counts[word] / len(document)\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                doc_counter += 1\n",
    "        idf = math.log(doc_num / doc_counter)\n",
    "        tf_idfs.append(tf*idf)\n",
    "    return tf_idfs\n",
    "\n",
    "def create_vector_arrays(training_data, corpus_dict, vocab, corpus, pos_tags):\n",
    "    len_feature_vector = len(vocab) + len(pos_tags) + 1\n",
    "    print(len_feature_vector)\n",
    "    vector_array = numpy.zeros((len(training_data), len_feature_vector))\n",
    "    results_array = numpy.zeros(len(training_data))\n",
    "    index = 0\n",
    "    for data, age in training_data.items():\n",
    "        results_array[index] = age\n",
    "        vector_array[index] = feature_vector(corpus_dict + data, vocab, corpus, pos_tags)\n",
    "        index += 1\n",
    "    return (vector_array, results_array)\n",
    "    \n",
    "    \n",
    "training_data = {'121-0.txt': 28, '158-0.txt':40}\n",
    "corpus = clean_and_tokenize_corpus(['data/JaneAusten/', 'data/CarrollLewis/', 'data/CharlesDickens/'])\n",
    "vocab = get_corpus_vocab(corpus, 200)\n",
    "corpus_dict = 'data/JaneAusten/'    \n",
    "\n",
    "tagset = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "\n",
    "create_vector_arrays(training_data, corpus_dict, vocab, corpus, tagset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

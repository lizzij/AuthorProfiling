{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import re\n",
    "import operator\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "\n",
    "nltk.internals.config_java(options='-Xmx3024m')\n",
    "st = StanfordPOSTagger('english-left3words-distsim.tagger', path_to_jar='stanford-postagger.jar')\n",
    "ner = StanfordNERTagger('english.all.3class.distsim.crf.ser.gz', 'stanford-ner.jar')\n",
    "\n",
    "def clean_and_tokenize_corpus(directories):\n",
    "    file_list = []\n",
    "    uncleaned_list = []\n",
    "    for directory in directories:\n",
    "        for file_name in os.listdir(directory):\n",
    "            if file_name != '.DS_Store':\n",
    "                file = open(directory + file_name)\n",
    "                raw_text = file.read()\n",
    "                raw_text_lowercase = raw_text.lower()\n",
    "                tokens = nltk.word_tokenize(raw_text_lowercase)\n",
    "                no_punct_tokens = [x for x in tokens if re.match('\\w+', x)]\n",
    "                file_list.append(no_punct_tokens)\n",
    "                uncleaned_list.append(nltk.word_tokenize(raw_text))\n",
    "                file.close()\n",
    "    return (file_list, uncleaned_list)\n",
    "\n",
    "def count_total_vocab(file_wordfreqs):\n",
    "    vocab = {}\n",
    "    for words in file_wordfreqs:\n",
    "        for word in words:\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def get_corpus_vocab(corpus, unknown_threshold):\n",
    "    initial_dict = count_total_vocab(corpus)\n",
    "    sorted_dict = sorted(initial_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    filtered_dict = [key for key, value in sorted_dict[:unknown_threshold]]\n",
    "    return filtered_dict\n",
    "\n",
    "def get_idf_dict(vocab, corpus):\n",
    "    idf_dict = {}\n",
    "    num_docs = len(corpus) \n",
    "    for word in vocab:\n",
    "        df_count = 1\n",
    "        for doc in corpus:\n",
    "            if word in doc:\n",
    "                df_count += 1\n",
    "        idf_dict[word] = math.log(num_docs / df_count)\n",
    "    return idf_dict\n",
    "\n",
    "def get_named_entities(novel):\n",
    "    named_entities = ner.tag(novel)\n",
    "    entities = []\n",
    "    cur_entity = []\n",
    "    last_tag = 'O'\n",
    "    for entity in named_entities:\n",
    "        if entity[1] == 'O':\n",
    "            if len(cur_entity) > 0:\n",
    "                new_entry = (' '.join(cur_entity), last_tag)\n",
    "                if new_entry not in entities:\n",
    "                    entities.append(new_entry)\n",
    "                cur_entity = []\n",
    "            last_tag = 'O'\n",
    "        elif entity[1] == last_tag:\n",
    "            cur_entity.append(entity[0])\n",
    "        else:\n",
    "            if len(cur_entity) > 0:\n",
    "                new_entry = (' '.join(cur_entity), last_tag)\n",
    "                if new_entry not in entities:\n",
    "                    entities.append(new_entry)\n",
    "                cur_entity = []\n",
    "            cur_entity.append(entity[0])\n",
    "            last_tag = entity[1]\n",
    "    return entities\n",
    "\n",
    "def get_ne_corpus(corpus):\n",
    "    return [get_named_entities(x) for x in corpus]\n",
    "    \n",
    "\n",
    "corpus = clean_and_tokenize_corpus(['data/JaneAusten/', 'data/CarrollLewis/', 'data/CharlesDickens/'])\n",
    "ugram_corpus = corpus[0]\n",
    "vocab = get_corpus_vocab(ugram_corpus, 500)\n",
    "idf_dict = get_idf_dict(vocab, ugram_corpus)\n",
    "ne_corpus = get_ne_corpus(corpus[1])\n",
    "ne_vocab = get_corpus_vocab(ne_corpus, 100)\n",
    "ne_idf_dict = get_idf_dict(ne_vocab, ne_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import os\n",
    "\n",
    "def train_word2vec(corpus):\n",
    "    model = Word2Vec(corpus, size=100)\n",
    "    model.train(corpus, total_examples=len(corpus), epochs=50)\n",
    "    return model\n",
    "\n",
    "word2vec = train_word2vec(ugram_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:41: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:42: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:52: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:53: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 4.11301321e-03,  2.98351949e-03,  2.89683440e-03,\n",
       "          3.04691604e-03,  1.98987553e-03,  1.65736707e-03,\n",
       "          1.63278473e-03,  1.39989943e-03,  1.44000746e-03,\n",
       "          2.01445787e-03,  1.03763340e-03,  7.03831137e-04,\n",
       "          1.16054509e-03,  6.79248800e-04,  1.41930654e-03,\n",
       "          8.84964150e-04,  9.09546488e-04,  1.34167811e-03,\n",
       "          8.56500391e-04,  1.02857675e-03,  9.35422633e-04,\n",
       "          6.62429306e-04,  7.62052463e-04,  6.35259354e-04,\n",
       "          6.88305450e-04,  5.29167160e-04,  4.47657304e-04,\n",
       "          3.58384605e-04,  2.32885303e-04,  6.11970823e-04,\n",
       "          4.43775882e-04,  6.14558438e-04,  2.62642869e-04,\n",
       "          4.02374051e-04,  4.29544003e-04,  3.77791713e-04,\n",
       "          4.46363497e-04,  5.54071709e-04,  4.89059136e-04,\n",
       "          4.94234365e-04,  3.73910292e-04,  3.99786436e-04,\n",
       "          3.79085521e-04,  4.70945834e-04,  3.44152725e-04,\n",
       "          3.23451809e-04,  2.62642869e-04,  2.65230484e-04,\n",
       "          2.30297688e-04,  3.16982773e-04,  2.65230484e-04,\n",
       "          3.40271303e-04,  2.80756170e-04,  2.74287134e-04,\n",
       "          2.54880026e-04,  2.83343785e-04,  2.08302965e-04,\n",
       "          3.07926122e-04,  2.94988050e-04,  2.22534845e-04,\n",
       "          1.60432097e-04,  1.50081640e-04,  2.97575665e-04,\n",
       "          2.21241038e-04,  2.10890580e-04,  2.45823375e-04,\n",
       "          4.77584649e-04,  2.60055255e-04,  2.04421543e-04,\n",
       "          2.65230484e-04,  1.88895857e-04,  1.30674531e-04,\n",
       "          1.15148844e-04,  1.85014435e-04,  1.65607326e-04,\n",
       "          1.97952507e-04,  2.09596772e-04,  1.94071086e-04,\n",
       "          2.66524291e-04,  2.69111905e-04,  2.04421543e-04,\n",
       "          2.03127736e-04,  2.00540122e-04,  2.13478194e-04,\n",
       "          1.85014435e-04,  1.78545399e-04,  1.35849760e-04,\n",
       "          1.12561230e-04,  1.08679808e-04,  1.48787832e-04,\n",
       "          1.78545399e-04,  1.72076363e-04,  1.13855037e-04,\n",
       "          1.44906411e-04,  1.47494025e-04,  1.47494025e-04,\n",
       "          1.11267422e-04,  1.68194941e-04,  1.07239686e-05,\n",
       "          1.20324073e-04,  1.50081640e-04,  1.11267422e-04,\n",
       "          1.31968338e-04,  1.15148844e-04,  1.24205495e-04,\n",
       "          1.16442651e-04,  1.43612603e-04,  1.13855037e-04,\n",
       "          1.33262145e-04,  7.63346270e-05,  1.08679808e-04,\n",
       "          7.37470125e-05,  1.43612603e-04,  4.91646750e-05,\n",
       "          1.77251592e-04,  1.07386001e-04,  8.28036632e-05,\n",
       "          6.59841691e-05,  1.55256868e-04,  1.42318796e-04,\n",
       "          4.26956388e-05,  1.34555953e-04,  2.32885303e-05,\n",
       "          8.28036632e-05,  1.12561230e-04,  9.83293500e-05,\n",
       "          1.28086916e-04,  1.29380724e-04,  1.30674531e-04,\n",
       "          1.13855037e-04,  9.44479283e-05,  1.19030266e-04,\n",
       "          1.08679808e-04,  9.31541211e-05,  1.12561230e-04,\n",
       "          1.12561230e-04,  7.24532053e-05,  9.31541211e-05,\n",
       "          6.98655908e-05,  1.13855037e-04,  6.59841691e-05,\n",
       "          5.30460967e-05,  9.83293500e-05,  8.40974704e-05,\n",
       "          7.50408198e-05,  1.17736459e-04,  7.11593981e-05,\n",
       "          5.95151329e-05,  4.14018316e-05,  6.21027474e-05,\n",
       "          1.06092193e-04,  7.11593981e-05,  6.08089402e-05,\n",
       "          8.02160487e-05,  6.33965546e-05,  3.23451809e-05,\n",
       "          5.43399040e-05,  5.04584823e-05,  6.46903619e-05,\n",
       "          7.37470125e-05,  5.04584823e-05,  6.08089402e-05,\n",
       "          7.37470125e-05,  6.85717836e-05,  5.43399040e-05,\n",
       "          7.76284342e-05,  8.15098560e-05,  3.88142171e-05,\n",
       "          6.33965546e-05,  7.63346270e-05,  8.92726994e-05,\n",
       "          8.92726994e-05,  1.20324073e-04,  6.85717836e-05,\n",
       "          1.03504579e-04,  1.09973615e-04,  1.28086916e-04,\n",
       "          7.50408198e-05,  6.33965546e-05,  7.63346270e-05,\n",
       "          5.17522895e-05,  3.10513737e-05,  7.11593981e-05,\n",
       "          1.40191891e-04,  9.41288410e-05,  0.00000000e+00,\n",
       "          1.37143567e-04,  7.24532053e-05,  5.56337112e-05,\n",
       "          4.26956388e-05,  7.50408198e-05,  1.94071086e-05,\n",
       "          4.52832533e-05,  5.56337112e-05,  4.65770605e-05,\n",
       "          4.14018316e-05,  5.69275184e-05,  7.11593981e-05,\n",
       "          4.91646750e-05,  9.83293500e-05,  2.84637592e-05,\n",
       "          7.37470125e-05,  5.69275184e-05,  6.33965546e-05,\n",
       "          5.17522895e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "          3.23451809e-05,  3.62266026e-05,  4.52832533e-05,\n",
       "          4.52832533e-05,  3.62266026e-05,  4.26956388e-05,\n",
       "          6.59841691e-05,  3.62266026e-05,  6.98655908e-05,\n",
       "          9.05665066e-05,  5.04584823e-05,  3.10513737e-05,\n",
       "          3.10513737e-05,  0.00000000e+00,  2.97575665e-05,\n",
       "          5.56337112e-05,  5.80794976e-05,  5.20712737e-05,\n",
       "          7.89222415e-05,  7.11593981e-05,  8.79788921e-05,\n",
       "          4.91646750e-05,  2.97575665e-05,  5.04584823e-05,\n",
       "          3.23451809e-05,  8.53912777e-05,  7.76284342e-05,\n",
       "          0.00000000e+00,  7.63346270e-05,  5.17522895e-05,\n",
       "          4.39894461e-05,  7.76284342e-05,  0.00000000e+00,\n",
       "          1.20678884e-05,  3.23451809e-05,  5.95151329e-05,\n",
       "          5.69275184e-05,  1.20164478e-04,  5.04584823e-05,\n",
       "          5.69275184e-05,  6.46903619e-05,  4.26956388e-05,\n",
       "          4.52832533e-05,  4.78708678e-05,  4.65770605e-05,\n",
       "          2.45823375e-05,  5.95151329e-05,  0.00000000e+00,\n",
       "          1.55256868e-04,  4.52832533e-05,  6.00822389e-06,\n",
       "          1.29380724e-05,  4.65770605e-05,  5.40740150e-05,\n",
       "          2.32885303e-05,  2.84637592e-05,  3.10513737e-05,\n",
       "          4.91646750e-05,  3.10513737e-05,  7.89222415e-05,\n",
       "          3.49327954e-05,  2.45823375e-05,  4.26956388e-05,\n",
       "          2.58761447e-05,  4.78708678e-05,  5.82879010e-03,\n",
       "          4.45389855e-06,  3.36389882e-05,  4.78708678e-05,\n",
       "          2.97575665e-05,  3.75204099e-05,  9.81343236e-05,\n",
       "          4.65770605e-05,  6.21027474e-05,  4.78708678e-05,\n",
       "          4.78708678e-05,  5.95151329e-05,  7.11593981e-05,\n",
       "          4.14018316e-05,  2.32885303e-05,  6.72779763e-05,\n",
       "          4.26956388e-05,  3.36389882e-05,  0.00000000e+00,\n",
       "          3.62266026e-05,  3.10513737e-05,  1.14388998e-04,\n",
       "          3.75204099e-05,  5.69275184e-05,  5.17522895e-05,\n",
       "          0.00000000e+00,  1.91483471e-04,  5.43399040e-05,\n",
       "          5.95151329e-05,  5.56337112e-05,  2.84637592e-05,\n",
       "          3.75204099e-05,  4.65770605e-05,  2.71699520e-05,\n",
       "          3.75204099e-05,  2.07009158e-05,  7.11593981e-05,\n",
       "          3.20438608e-05,  0.00000000e+00,  7.81069106e-05,\n",
       "          4.91646750e-05,  3.10513737e-05,  3.88142171e-05,\n",
       "          3.49327954e-05,  5.17522895e-06,  4.14018316e-05,\n",
       "          6.23545797e-05,  8.41151345e-05,  3.62266026e-05,\n",
       "          4.01080244e-05,  4.65770605e-05,  2.32885303e-05,\n",
       "          2.84637592e-05,  3.30435739e-05,  3.23451809e-05,\n",
       "          5.04584823e-05,  5.69275184e-05,  7.20986867e-05,\n",
       "          2.00274130e-05,  2.58761447e-05,  2.00274130e-05,\n",
       "          3.40466021e-05,  2.14479371e-05,  8.81206171e-05,\n",
       "          6.00822389e-05,  2.00274130e-05,  2.97575665e-05,\n",
       "          2.45823375e-05,  3.75204099e-05,  3.36389882e-05,\n",
       "          3.23451809e-05,  4.65770605e-05,  3.36389882e-05,\n",
       "          2.84637592e-05,  4.65770605e-05,  2.58761447e-05,\n",
       "          1.02142382e-04,  2.97575665e-05,  3.62266026e-05,\n",
       "          1.42318796e-05,  2.58761447e-05,  2.71699520e-05,\n",
       "          1.94071086e-05,  1.68194941e-05,  2.07009158e-05,\n",
       "          9.41288410e-05,  3.62266026e-05,  1.80246717e-05,\n",
       "          1.94071086e-05,  4.65770605e-05,  2.07009158e-05,\n",
       "          6.21027474e-05,  6.60904628e-05,  3.75204099e-05,\n",
       "          5.60767563e-05,  0.00000000e+00,  3.75204099e-05,\n",
       "          2.07009158e-05,  7.20986867e-05,  4.14018316e-05,\n",
       "          2.32885303e-05,  3.88142171e-05,  1.16442651e-05,\n",
       "          2.97575665e-05,  8.41151345e-05,  1.02142382e-04,\n",
       "          6.00822389e-05,  6.40877215e-05,  3.49327954e-05,\n",
       "          6.20849802e-05,  1.94071086e-05,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  2.58761447e-05,\n",
       "          3.10513737e-05,  5.82213257e-05,  4.14018316e-05,\n",
       "          4.40603086e-05,  2.32885303e-05,  0.00000000e+00,\n",
       "          2.45823375e-05,  2.97575665e-05,  2.97575665e-05,\n",
       "          1.08193788e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "          3.88142171e-05,  2.45823375e-05,  2.84637592e-05,\n",
       "          1.80246717e-05,  1.60219304e-05,  4.01080244e-05,\n",
       "          2.32885303e-05,  1.94071086e-05,  0.00000000e+00,\n",
       "          2.19947230e-05,  1.94071086e-05,  1.35269640e-04,\n",
       "          2.97575665e-05,  2.20301543e-05,  1.94071086e-05,\n",
       "          0.00000000e+00,  1.29380724e-05,  5.60767563e-05,\n",
       "          2.71699520e-05,  1.24709159e-04,  1.03504579e-05,\n",
       "          4.65770605e-05,  0.00000000e+00,  0.00000000e+00,\n",
       "          2.84637592e-05,  1.42318796e-05,  3.49327954e-05,\n",
       "          0.00000000e+00,  2.60356369e-05,  2.32885303e-05,\n",
       "          3.49327954e-05,  3.75204099e-05,  4.14018316e-05,\n",
       "          2.45823375e-05,  0.00000000e+00,  3.10513737e-05,\n",
       "          2.45823375e-05,  3.62266026e-05,  1.55256868e-05,\n",
       "          3.20438608e-05,  2.84637592e-05,  1.78155942e-05,\n",
       "          0.00000000e+00,  4.00548260e-06,  6.07333079e-05,\n",
       "          2.97575665e-05,  7.61398986e-06,  2.97575665e-05,\n",
       "          3.75204099e-05,  6.20849802e-05,  8.22170922e-05,\n",
       "          6.07333079e-05,  1.55256868e-05,  1.42318796e-05,\n",
       "          2.32885303e-05,  2.58761447e-05,  0.00000000e+00,\n",
       "          1.55256868e-05,  2.45823375e-05,  4.00548260e-05,\n",
       "          2.58761447e-05,  4.26956388e-05,  1.03745948e-05,\n",
       "          3.30435739e-05,  9.05665066e-06,  2.40328956e-05,\n",
       "          1.20164478e-05,  1.35836935e-04,  5.40740150e-05,\n",
       "          1.81133013e-05,  5.00685324e-05,  4.80657911e-05,\n",
       "          1.68194941e-05,  2.19947230e-05,  4.40603086e-05,\n",
       "          3.49327954e-05,  0.00000000e+00,  1.80246717e-05,\n",
       "          0.00000000e+00,  1.62290682e-05,  1.81133013e-05,\n",
       "          1.06893565e-04,  3.60493434e-05,  3.49327954e-05,\n",
       "          1.16442651e-05,  5.60767563e-05,  1.93242343e-05,\n",
       "          0.00000000e+00,  0.00000000e+00,  2.07009158e-05,\n",
       "          1.81133013e-05,  0.00000000e+00,  1.42318796e-05,\n",
       "          6.07333079e-05,  1.06893565e-03,  2.07009158e-05,\n",
       "          4.69302834e-05,  7.45363325e-05,  0.00000000e+00,\n",
       "          2.45823375e-05,  0.00000000e+00,  3.57754833e-02,\n",
       "          9.97363796e-03,  7.34292619e-02,  1.90026362e-03,\n",
       "          2.15289982e-03,  1.06887083e-01,  5.69969244e-02,\n",
       "          2.77899824e-03,  2.14191564e-03,  0.00000000e+00,\n",
       "          1.94200351e-02,  1.11170914e-01,  2.30338313e-02,\n",
       "          3.30184534e-02,  7.46924429e-04,  2.32864675e-03,\n",
       "          4.65729350e-03,  7.39125659e-02,  3.08106327e-02,\n",
       "          6.77943761e-02,  1.82337434e-03,  1.03251318e-03,\n",
       "          2.83391916e-03,  2.45935852e-02,  2.63620387e-03,\n",
       "          4.14433216e-02,  5.10654657e-02,  1.55206503e-02,\n",
       "          2.49231107e-02,  1.70035149e-02,  9.31458699e-03,\n",
       "          5.08567663e-03,  4.49253076e-03,  3.18541301e-04,\n",
       "          4.78910369e-03, -1.32556781e-01,  2.17231348e-01,\n",
       "          1.84277207e-01,  3.36169630e-01,  6.49212748e-02,\n",
       "         -2.39035934e-01,  7.51488358e-02, -1.66500900e-02,\n",
       "          1.35290965e-01,  4.86065477e-01, -6.63615903e-03,\n",
       "         -2.41974607e-01, -1.60969183e-01, -1.90267092e-05,\n",
       "          6.98189735e-02, -2.49405175e-01,  2.96697706e-01,\n",
       "         -7.74384260e-01, -4.10978973e-01, -2.96136230e-01,\n",
       "         -9.77301151e-02,  1.36262894e-01, -1.77487537e-01,\n",
       "          1.63521245e-01, -2.59021044e-01,  3.20754945e-02,\n",
       "          1.64475530e-01,  2.20750093e-01, -2.28634566e-01,\n",
       "          9.53251198e-02, -2.26339325e-02,  1.49055034e-01,\n",
       "          4.09704484e-02,  4.11656797e-01,  2.72938088e-02,\n",
       "          8.71691555e-02,  2.54521132e-01, -2.16689378e-01,\n",
       "          1.69812009e-01, -2.46660456e-01, -1.58865154e-01,\n",
       "          1.99976593e-01, -6.36720136e-02,  7.73528144e-02,\n",
       "          2.00852770e-02,  1.07963264e-01,  3.29553150e-02,\n",
       "         -2.54832357e-01,  1.45339638e-01,  3.51598449e-02,\n",
       "         -1.44909889e-01, -4.45749789e-01, -5.69175184e-03,\n",
       "         -5.31971045e-02, -2.12327570e-01, -7.61411414e-02,\n",
       "         -2.45012511e-02,  8.62911716e-02, -2.63919353e-01,\n",
       "          5.22768982e-02,  2.84782350e-01, -1.08419038e-01,\n",
       "         -4.56901103e-01, -1.00041486e-01,  2.65571803e-01,\n",
       "         -1.11842446e-01,  8.66747200e-02, -1.90290228e-01,\n",
       "          7.14336149e-03, -8.93625617e-02,  4.55207229e-01,\n",
       "         -5.03757261e-02,  2.52066284e-01,  3.11184376e-01,\n",
       "         -1.85177311e-01,  8.85437503e-02, -1.11333646e-01,\n",
       "          2.83868283e-01, -1.42267272e-01, -1.15383446e-01,\n",
       "         -1.27303183e-01,  1.38461426e-01, -2.58312136e-01,\n",
       "         -8.47303197e-02, -3.48863244e-01,  2.10373074e-01,\n",
       "          1.13302544e-01,  2.07671508e-01, -2.77992878e-02,\n",
       "          2.25938261e-01,  1.52071295e-02,  1.66065171e-01,\n",
       "         -3.04848731e-01,  8.75068307e-02,  1.92981780e-01,\n",
       "         -1.78283036e-01, -2.87000656e-01, -9.89598259e-02,\n",
       "         -2.63190359e-01,  1.51021972e-01,  7.19385738e-02,\n",
       "          3.04666877e-01,  6.46399472e+01,  6.50735004e-04,\n",
       "          6.50735004e-04,  3.24744014e-03,  3.24744014e-03,\n",
       "          3.82954243e-03,  3.82954243e-03,  3.82954243e-03,\n",
       "          3.82954243e-03,  4.48027744e-03,  4.48027744e-03,\n",
       "          0.00000000e+00,  5.21801994e-03,  5.21801994e-03,\n",
       "          5.21801994e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  6.06968115e-03,\n",
       "          6.06968115e-03,  6.06968115e-03,  6.06968115e-03,\n",
       "          6.06968115e-03,  6.06968115e-03,  6.06968115e-03,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          7.07698257e-03,  7.07698257e-03,  7.07698257e-03,\n",
       "          7.07698257e-03,  7.07698257e-03,  7.07698257e-03,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          8.30981987e-03,  8.30981987e-03,  8.30981987e-03,\n",
       "          8.30981987e-03,  8.30981987e-03,  8.30981987e-03,\n",
       "          8.30981987e-03,  8.30981987e-03,  8.30981987e-03,\n",
       "          8.30981987e-03,  8.30981987e-03,  8.30981987e-03,\n",
       "          8.30981987e-03,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00,  0.00000000e+00,  0.00000000e+00]]), array([28.]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "import numpy\n",
    "from nltk import word_tokenize\n",
    "from numpy import array\n",
    "from collections import Counter\n",
    "import math\n",
    "import readability\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def clean_and_tokenize_file(file_name):\n",
    "    file = open(file_name)\n",
    "    raw_text = file.read()\n",
    "    raw_text_lower = raw_text.lower()\n",
    "    caseless_tokens = nltk.word_tokenize(raw_text_lower)\n",
    "    sentences = nltk.sent_tokenize(raw_text_lower)\n",
    "    raw_sentence_tokens = nltk.sent_tokenize(raw_text)\n",
    "    case_tokens = nltk.word_tokenize(raw_text)\n",
    "    sentence_list = []\n",
    "    for sentence in sentences:\n",
    "        words = nltk.word_tokenize(sentence)\n",
    "        sentence_list.append([x for x in words if re.match('\\w+', x)])\n",
    "    return (caseless_tokens, sentence_list, raw_sentence_tokens, case_tokens)\n",
    "\n",
    "\n",
    "def tf_idf(vocab, document, idf_dict):\n",
    "    counts = Counter(document)\n",
    "    doc_len = len(document)\n",
    "    doc_num = len(corpus)\n",
    "    tf_idfs = []\n",
    "    for word in vocab:\n",
    "        doc_counter = 0\n",
    "        tf = counts[word] / len(document)\n",
    "        idf = idf_dict[word]\n",
    "        tf_idfs.append(tf*idf)\n",
    "    return tf_idfs\n",
    "\n",
    "def get_avg_word2vec(novel):\n",
    "    vec_list = []\n",
    "    for word in novel:\n",
    "        if word in word2vec:\n",
    "            vec_list.append(word2vec[word])\n",
    "    vector_array = numpy.array(vec_list)\n",
    "    vector_array = numpy.mean(vector_array, axis=0)\n",
    "    return vector_array.tolist()\n",
    "\n",
    "def get_pairwise_similarity(sentences):\n",
    "    sentence_vectors = []\n",
    "    for sentence in sentences:\n",
    "        vec_list = []\n",
    "        for word in sentence:\n",
    "            if word in word2vec:\n",
    "                vec_list.append(word2vec[word])\n",
    "        if len(vec_list) > 0:\n",
    "            vector_array = numpy.array(vec_list)\n",
    "            vector_array = numpy.mean(vector_array, axis=0)\n",
    "            sentence_vectors.append(vector_array)\n",
    "    cs_sim = cosine_similarity(sentence_vectors, sentence_vectors)\n",
    "    return numpy.mean(cs_sim)\n",
    "\n",
    "def get_pos_vector(novel, pos_tags):\n",
    "    tagged_tokens = st.tag(novel)\n",
    "    tags = [tag for (word, tag) in tagged_tokens]\n",
    "    num_tokens = len(novel)\n",
    "    pos_vector = [tags.count(pos_tag) / num_tokens for pos_tag in pos_tags]\n",
    "    return pos_vector\n",
    "\n",
    "def get_readability_score(sentences):\n",
    "    newline_regex = re.compile(r'\\n')\n",
    "    clean_sents = []\n",
    "    for sentence in sentences:\n",
    "        clean_sents.append(re.sub(newline_regex, ' ', sentence))\n",
    "    readability_text = '\\n'.join(clean_sents)\n",
    "    readability_results = readability.getmeasures(readability_text, lang='en')\n",
    "    flesch_score = readability_results['readability grades']['FleschReadingEase']   \n",
    "    return flesch_score\n",
    "\n",
    "def feature_vector(novel, vocab, idf_dict, pos_tags):\n",
    "    tokens = novel[0] # for unigrams\n",
    "    token_sents = novel[1] #  for sent similarity\n",
    "    case_tokens = novel[3] # for pos tagging and ner\n",
    "    \n",
    "    unigram_vector = tf_idf(vocab, tokens, idf_dict)\n",
    "    pos_vector = get_pos_vector(case_tokens, pos_tags)\n",
    "    word_2_vec_vector = get_avg_word2vec(tokens)\n",
    "    word_vector = unigram_vector + pos_vector + word_2_vec_vector\n",
    "    \n",
    "    # gets ratio of unique words to total # of words\n",
    "    novel_vocab = count_total_vocab([tokens])\n",
    "    vocab_size = len(novel_vocab) / len(tokens)\n",
    "    word_vector.append(vocab_size)\n",
    "      \n",
    "    cs_similarity = get_pairwise_similarity(token_sents)\n",
    "    word_vector.append(cs_similarity)\n",
    "    \n",
    "    return word_vector\n",
    "\n",
    "def feature_vector_time_period(novel, vocab, idf_dict, pos_tags, ne_vocab, ne_novel, ne_idf_dict):\n",
    "    word_vector = feature_vector(novel, vocab, idf_dict, pos_tags)\n",
    "    raw_sents = novel[2]\n",
    "    word_vector.append(get_readability_score(raw_sents))\n",
    "    ne_vector = tf_idf(ne_vocab, ne_novel, ne_idf_dict)\n",
    "    time_period_vector = word_vector + ne_vector\n",
    "    return time_period_vector\n",
    "\n",
    "def create_vector_arrays(training_data, corpus_dict, vocab, idf_dict, pos_tags, is_time, ner_vocab, ner_corpus, ner_idf_dict):\n",
    "    if is_time:\n",
    "        len_feature_vector = len(vocab) + len(pos_tags) + len(ner_vocab) + 103\n",
    "    else:\n",
    "        len_feature_vector = len(vocab) + len(pos_tags) + 102\n",
    "    vector_array = numpy.zeros((len(training_data), len_feature_vector))\n",
    "    results_array = numpy.zeros(len(training_data))\n",
    "    index = 0\n",
    "    for data, age in training_data.items():\n",
    "        results_array[index] = age\n",
    "        novel = clean_and_tokenize_file(corpus_dict + data)\n",
    "        if is_time:\n",
    "            vector_array[index] = feature_vector_time_period(novel, vocab, idf_dict, pos_tags, ner_vocab, ner_corpus[index], ner_idf_dict)\n",
    "        else:\n",
    "            vector_array[index] = feature_vector(novel, vocab, idf_dict, pos_tags)\n",
    "        index += 1\n",
    "    return (vector_array, results_array)\n",
    "    \n",
    "    \n",
    "training_data = {'121-0.txt': 28}\n",
    "\n",
    "corpus_dict = 'data/JaneAusten/'    \n",
    "tagset = ['CC', 'CD', 'DT', 'EX', 'FW', 'IN', 'JJ', 'JJR', 'JJS', 'LS', 'MD', 'NN', 'NNS', 'NNP', 'NNPS', 'PDT', 'POS', 'PRP', 'PRP$', 'RB', 'RBR', 'RBS', 'RP', 'TO', 'UH', 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ', 'WDT', 'WP', 'WP$', 'WRB']\n",
    "\n",
    "create_vector_arrays(training_data, corpus_dict, vocab, idf_dict, tagset, True, ne_vocab, ne_corpus, ne_idf_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from torch import tensor, Size\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch import functional\n",
    "from torch import from_numpy\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "input_array = array(features_vector_array[0], dtype='float32')\n",
    "input_vector = from_numpy(input_array)\n",
    "\n",
    "output_array = array(features_vector_array[1], dtype='float32')\n",
    "target_vector = from_numpy(output_array)\n",
    "\n",
    "input_and_target = TensorDataset(input_vector, target_vector)\n",
    "\n",
    "predict_array = array(test_vector, dtype='float32')\n",
    "predict_vector = from_numpy(predict_array)\n",
    "\n",
    "dickens_predict_array = array(dickens_test_array, dtype='float32')\n",
    "dickens_predict_vector = from_numpy(dickens_predict_array)\n",
    "\n",
    "linear_model = nn.Linear(115, 1)\n",
    "loss_func = nn.functional.mse_loss \n",
    "optimize = optim.SGD(linear_model.parameters(), lr=0.001)\n",
    "loss = loss_func(linear_model(input_vector), target_vector)\n",
    "\n",
    "def train(num_epochs, model, loss_func, optimizer):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in input_and_target:\n",
    "            \n",
    "            # Predictions\n",
    "            pred = model(xb)\n",
    "            loss = loss_func(pred, yb)\n",
    "            \n",
    "            # Stochastic radient descent\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "    print(loss)\n",
    "\n",
    "train(500, linear_model, loss_func, optimize)\n",
    "\n",
    "pred = linear_model(predict_vector)\n",
    "print(pred)\n",
    "\n",
    "def goodness(predicted, actual, range_size):\n",
    "    return float((predicted - actual)) / float(range_size)\n",
    "\n",
    "print(\"Jane Austen\")\n",
    "print(\"Age predicted for Mansfield Park: %s\" % pred)\n",
    "print(\"Goodness Metric: %s\\n\" % abs(goodness(pred, 39, 15)))\n",
    "\n",
    "pred = linear_model(dickens_predict_vector)\n",
    "print(\"Charles Dickens\")\n",
    "print(\"Age predicted for Oliver Twist: %s\" % pred)\n",
    "print(\"Goodness Metric: %s\" % abs(goodness(pred, 25, 10)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
